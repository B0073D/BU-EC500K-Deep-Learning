{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Problem Set 4\n",
    "Designed by Ben Usman, Kun He, and Sarah Adel Bargal, with help from Kate Saenko and Brian Kulis.\n",
    "\n",
    "This assignment will introduce you to:\n",
    "1. Building and training a convolutional network\n",
    "2. Saving snapshots of your trained model\n",
    "3. Reloading weights from a saved model\n",
    "4. Fine-tuning a pre-trained network\n",
    "5. Visualizations using Tensorboard\n",
    "\n",
    "This code has been tested and should for Python 3.5 and 2.7 with tensorflow 1.0.*. Since recently, you can update to recent tensorflow version just by doing `pip install tensorflow`,  or `pip install tensorflow-gpu` if you want to use GPU.\n",
    "\n",
    "**Note:** This notebook contains problem descriptions and demo/starter code. However, you're welcome to implement and submit .py files directly, if that's easier for you. Starter .py files are provided in the same `pset4/` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 1: Building and Training a ConvNet on SVHN\n",
    "(25 points)\n",
    "\n",
    "First we provide demo code that trains a convolutional network on the [SVHN Dataset](http://ufldl.stanford.edu/housenumbers/).. \n",
    "\n",
    "You will need to download   __Format 2__ from the link above.\n",
    "- Create a directory named `svhn_mat/` in the working directory. Or, you can create it anywhere you want, but change the path in `svhn_dataset_generator` to match it.\n",
    "- Download `train_32x32.mat` and `test_32x32.mat` to this directory.\n",
    "- `extra_32x32.mat` is NOT needed.\n",
    "- You may find the `wget` command useful for downloading on linux. \n",
    "\n",
    "\n",
    "\n",
    "The following defines a generator for the SVHN Dataset, yielding the next batch every time next is invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "from six.moves import range\n",
    "import read_data\n",
    "from time import time\n",
    "\n",
    "@read_data.restartable\n",
    "def svhn_dataset_generator(dataset_name, batch_size):\n",
    "    assert dataset_name in ['train', 'test']\n",
    "    assert batch_size > 0 or batch_size == -1  # -1 for entire dataset\n",
    "    \n",
    "    path = './svhn_mat/' # path to the SVHN dataset you will download in Q1.1\n",
    "    file_name = '%s_32x32.mat' % dataset_name\n",
    "    file_dict = scipy.io.loadmat(os.path.join(path, file_name))\n",
    "    X_all = file_dict['X'].transpose((3, 0, 1, 2))\n",
    "    y_all = file_dict['y']\n",
    "    data_len = X_all.shape[0]\n",
    "    batch_size = batch_size if batch_size > 0 else data_len\n",
    "    \n",
    "    X_all_padded = np.concatenate([X_all, X_all[:batch_size]], axis=0)\n",
    "    y_all_padded = np.concatenate([y_all, y_all[:batch_size]], axis=0)\n",
    "    y_all_padded[y_all_padded == 10] = 0\n",
    "    \n",
    "    for slice_i in range(int(math.ceil(data_len / batch_size))):\n",
    "        idx = slice_i * batch_size\n",
    "        X_batch = X_all_padded[idx:idx + batch_size]\n",
    "        y_batch = np.ravel(y_all_padded[idx:idx + batch_size])\n",
    "        yield X_batch, y_batch\n",
    "        \n",
    "import tensorflow as tf\n",
    "\n",
    "def cnn_map(x_):\n",
    "    conv1 = tf.layers.conv2d(\n",
    "            inputs=x_,\n",
    "            filters=32,\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu,\n",
    "            name='conv1')\n",
    "    \n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, \n",
    "                                    pool_size=[2, 2], \n",
    "                                    strides=2)\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(\n",
    "            inputs=pool1,\n",
    "            filters=32,\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu,\n",
    "            name='conv2')\n",
    "    \n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, \n",
    "                                    pool_size=[2, 2], \n",
    "                                    strides=2)\n",
    "        \n",
    "    pool_flat = tf.contrib.layers.flatten(pool2, scope='pool2flat')\n",
    "    dense = tf.layers.dense(inputs=pool_flat, units=500, activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(inputs=dense, units=10)\n",
    "    return logits\n",
    "\n",
    "\n",
    "def apply_classification_loss(model_function):\n",
    "    with tf.Graph().as_default() as g:\n",
    "        with tf.device(\"/gpu:0\"):\n",
    "            x_ = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "            y_ = tf.placeholder(tf.int32, [None])\n",
    "            y_logits = model_function(x_)\n",
    "            \n",
    "            y_dict = dict(labels=y_, logits=y_logits)\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(**y_dict)\n",
    "            cross_entropy_loss = tf.reduce_mean(losses)\n",
    "            trainer = tf.train.AdamOptimizer()\n",
    "            train_op = trainer.minimize(cross_entropy_loss)\n",
    "            \n",
    "            y_pred = tf.argmax(tf.nn.softmax(y_logits), dimension=1)\n",
    "            correct_prediction = tf.equal(tf.cast(y_pred, tf.int32), y_)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = 'accuracy')\n",
    "    \n",
    "    model_dict = {'graph': g, 'inputs': [x_, y_], 'train_op': train_op,\n",
    "                  'accuracy': accuracy, 'loss': cross_entropy_loss}\n",
    "    \n",
    "    return model_dict\n",
    "\n",
    "def train_model(model_dict, dataset_generators, epoch_n, print_every=287):\n",
    "    with model_dict['graph'].as_default(), tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        acc_tmp = 0.0\n",
    "        for epoch_i in range(epoch_n):\n",
    "            for iter_i, data_batch in enumerate(dataset_generators['train']):\n",
    "                train_feed_dict = dict(zip(model_dict['inputs'], data_batch))\n",
    "                sess.run(model_dict['train_op'], feed_dict=train_feed_dict)\n",
    "                \n",
    "                if iter_i % print_every == print_every-1:\n",
    "                    collect_arr = []\n",
    "                    for test_batch in dataset_generators['test']:\n",
    "                        test_feed_dict = dict(zip(model_dict['inputs'], test_batch))\n",
    "                        to_compute = [model_dict['loss'], model_dict['accuracy']]\n",
    "                        collect_arr.append(sess.run(to_compute, test_feed_dict))\n",
    "                    averages = np.mean(collect_arr, axis=0)\n",
    "                    fmt = (epoch_i+1, print_every) + tuple(averages)\n",
    "                    print('epoch {:d}, iter: {:d}, loss: {:.3f}, '\n",
    "                          'accuracy: {:.3f}'.format(*fmt))\n",
    "                    \n",
    "            # Early stopping with patience of 3 epoches\n",
    "            if averages[1] < acc_tmp:\n",
    "                patience_ct += 1\n",
    "                if patience_ct == 3:\n",
    "                    print('Early stopping!'); break\n",
    "            else: patience_ct = 0\n",
    "            loss_tmp = averages[0]; acc_tmp = averages[1]\n",
    "            \n",
    "dataset_generators = {\n",
    "        'train': svhn_dataset_generator('train', 256),\n",
    "        'test': svhn_dataset_generator('test', 256)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, iter: 287, loss: 1.155, accuracy: 0.648\n",
      "epoch 2, iter: 287, loss: 0.805, accuracy: 0.765\n",
      "epoch 3, iter: 287, loss: 0.700, accuracy: 0.804\n",
      "epoch 4, iter: 287, loss: 0.689, accuracy: 0.812\n",
      "epoch 5, iter: 287, loss: 0.740, accuracy: 0.805\n",
      "epoch 6, iter: 287, loss: 0.733, accuracy: 0.812\n",
      "epoch 7, iter: 287, loss: 0.741, accuracy: 0.818\n",
      "epoch 8, iter: 287, loss: 0.784, accuracy: 0.817\n",
      "epoch 9, iter: 287, loss: 0.814, accuracy: 0.823\n",
      "epoch 10, iter: 287, loss: 0.818, accuracy: 0.825\n",
      "epoch 11, iter: 287, loss: 0.939, accuracy: 0.808\n",
      "epoch 12, iter: 287, loss: 1.042, accuracy: 0.797\n",
      "epoch 13, iter: 287, loss: 0.977, accuracy: 0.815\n",
      "epoch 14, iter: 287, loss: 1.046, accuracy: 0.823\n",
      "epoch 15, iter: 287, loss: 1.143, accuracy: 0.807\n",
      "epoch 16, iter: 287, loss: 1.092, accuracy: 0.819\n",
      "epoch 17, iter: 287, loss: 1.085, accuracy: 0.826\n",
      "epoch 18, iter: 287, loss: 1.272, accuracy: 0.820\n",
      "epoch 19, iter: 287, loss: 1.296, accuracy: 0.820\n",
      "epoch 20, iter: 287, loss: 1.346, accuracy: 0.820\n",
      "epoch 21, iter: 287, loss: 1.357, accuracy: 0.822\n",
      "epoch 22, iter: 287, loss: 1.456, accuracy: 0.823\n",
      "epoch 23, iter: 287, loss: 1.521, accuracy: 0.821\n",
      "epoch 24, iter: 287, loss: 1.571, accuracy: 0.822\n",
      "epoch 25, iter: 287, loss: 1.729, accuracy: 0.813\n",
      "epoch 26, iter: 287, loss: 1.802, accuracy: 0.815\n",
      "epoch 27, iter: 287, loss: 1.916, accuracy: 0.810\n",
      "epoch 28, iter: 287, loss: 1.881, accuracy: 0.816\n",
      "epoch 29, iter: 287, loss: 2.152, accuracy: 0.809\n",
      "epoch 30, iter: 287, loss: 2.110, accuracy: 0.812\n",
      "epoch 31, iter: 287, loss: 2.166, accuracy: 0.813\n",
      "epoch 32, iter: 287, loss: 2.091, accuracy: 0.814\n",
      "epoch 33, iter: 287, loss: 2.104, accuracy: 0.818\n",
      "epoch 34, iter: 287, loss: 2.265, accuracy: 0.820\n",
      "epoch 35, iter: 287, loss: 2.356, accuracy: 0.828\n",
      "epoch 36, iter: 287, loss: 2.363, accuracy: 0.819\n",
      "epoch 37, iter: 287, loss: 2.342, accuracy: 0.827\n",
      "epoch 38, iter: 287, loss: 2.294, accuracy: 0.828\n",
      "epoch 39, iter: 287, loss: 2.363, accuracy: 0.827\n",
      "epoch 40, iter: 287, loss: 2.522, accuracy: 0.821\n",
      "epoch 41, iter: 287, loss: 2.633, accuracy: 0.826\n",
      "epoch 42, iter: 287, loss: 2.578, accuracy: 0.828\n",
      "epoch 43, iter: 287, loss: 2.701, accuracy: 0.829\n",
      "epoch 44, iter: 287, loss: 2.616, accuracy: 0.829\n",
      "epoch 45, iter: 287, loss: 2.983, accuracy: 0.834\n",
      "epoch 46, iter: 287, loss: 3.075, accuracy: 0.832\n",
      "epoch 47, iter: 287, loss: 2.908, accuracy: 0.825\n",
      "epoch 48, iter: 287, loss: 3.158, accuracy: 0.837\n",
      "epoch 49, iter: 287, loss: 3.000, accuracy: 0.833\n",
      "epoch 50, iter: 287, loss: 3.191, accuracy: 0.829\n"
     ]
    }
   ],
   "source": [
    "model_dict = apply_classification_loss(cnn_map)\n",
    "# I used early stopping with patience of 3 epoches on both loss and accuracy.\n",
    "train_model(model_dict, dataset_generators, epoch_n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Q1.3 SVHN Net Variations\n",
    "Now we vary the structure of the network. To keep things simple, we still use  two identical conv layers, but vary their parameters. \n",
    "\n",
    "Report the final test accuracy on 3 different number of filters, and 3 different number of strides. Each time when you vary one parameter, keep the other fixed at the original value.\n",
    "\n",
    "|Stride|Accuracy|\n",
    "|--|-------------------------------|\n",
    "| 3 | 0.834 |\n",
    "| 4 | 0.825 |\n",
    "| 5 | 0.807 |\n",
    "\n",
    "|Filters|Accuracy|\n",
    "|--|-------------------------------|\n",
    "| 28 | 0.850 |\n",
    "| 36 | 0.772 |\n",
    "| 40 | 0.826 |\n",
    "\n",
    "A template for one sample modification is given below. \n",
    "\n",
    "**Note:** you're welcome to decide how many training epochs to use, if that gets you the same results but faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_modification(x_, filters, strides):\n",
    "    conv1 = tf.layers.conv2d(\n",
    "            inputs=x_,\n",
    "            filters=filters,\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu,\n",
    "            name='conv1')\n",
    "    \n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, \n",
    "                                    pool_size=[2, 2], \n",
    "                                    strides=strides)\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(\n",
    "            inputs=pool1,\n",
    "            filters=filters,\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu,\n",
    "            name='conv2')\n",
    "    \n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, \n",
    "                                    pool_size=[2, 2], \n",
    "                                    strides=strides)\n",
    "        \n",
    "    pool_flat = tf.contrib.layers.flatten(pool2, scope='pool2flat')\n",
    "    dense = tf.layers.dense(inputs=pool_flat, units=500, activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(inputs=dense, units=10)\n",
    "    return logits\n",
    "\n",
    "def apply_classification_loss_modification(model_function, cnn_input):\n",
    "    with tf.Graph().as_default() as g:\n",
    "        with tf.device(\"/gpu:0\"):\n",
    "            x_ = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "            y_ = tf.placeholder(tf.int32, [None])\n",
    "            y_logits = model_function(x_, *cnn_input)\n",
    "            \n",
    "            y_dict = dict(labels=y_, logits=y_logits)\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(**y_dict)\n",
    "            cross_entropy_loss = tf.reduce_mean(losses)\n",
    "            trainer = tf.train.AdamOptimizer()\n",
    "            train_op = trainer.minimize(cross_entropy_loss)\n",
    "            \n",
    "            y_pred = tf.argmax(tf.nn.softmax(y_logits), dimension=1)\n",
    "            correct_prediction = tf.equal(tf.cast(y_pred, tf.int32), y_)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = 'accuracy')\n",
    "    \n",
    "    model_dict = {'graph': g, 'inputs': [x_, y_], 'train_op': train_op,\n",
    "                  'accuracy': accuracy, 'loss': cross_entropy_loss}\n",
    "    \n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter size = 32, stride size = 3\n",
      "epoch 1, iter: 287, loss: 1.491, accuracy: 0.519\n",
      "epoch 2, iter: 287, loss: 1.015, accuracy: 0.693\n",
      "epoch 3, iter: 287, loss: 0.910, accuracy: 0.732\n",
      "epoch 4, iter: 287, loss: 0.813, accuracy: 0.763\n",
      "epoch 5, iter: 287, loss: 0.782, accuracy: 0.771\n",
      "epoch 6, iter: 287, loss: 0.732, accuracy: 0.791\n",
      "epoch 7, iter: 287, loss: 0.716, accuracy: 0.800\n",
      "epoch 8, iter: 287, loss: 0.731, accuracy: 0.800\n",
      "epoch 9, iter: 287, loss: 0.712, accuracy: 0.806\n",
      "epoch 10, iter: 287, loss: 0.711, accuracy: 0.814\n",
      "epoch 11, iter: 287, loss: 0.788, accuracy: 0.803\n",
      "epoch 12, iter: 287, loss: 0.831, accuracy: 0.797\n",
      "epoch 13, iter: 287, loss: 0.829, accuracy: 0.801\n",
      "epoch 14, iter: 287, loss: 0.875, accuracy: 0.804\n",
      "epoch 15, iter: 287, loss: 0.859, accuracy: 0.804\n",
      "epoch 16, iter: 287, loss: 0.825, accuracy: 0.824\n",
      "epoch 17, iter: 287, loss: 0.828, accuracy: 0.827\n",
      "epoch 18, iter: 287, loss: 0.904, accuracy: 0.824\n",
      "epoch 19, iter: 287, loss: 0.889, accuracy: 0.826\n",
      "epoch 20, iter: 287, loss: 0.999, accuracy: 0.814\n",
      "epoch 21, iter: 287, loss: 1.030, accuracy: 0.818\n",
      "epoch 22, iter: 287, loss: 1.078, accuracy: 0.821\n",
      "epoch 23, iter: 287, loss: 0.983, accuracy: 0.828\n",
      "epoch 24, iter: 287, loss: 1.102, accuracy: 0.826\n",
      "epoch 25, iter: 287, loss: 1.082, accuracy: 0.824\n",
      "epoch 26, iter: 287, loss: 1.173, accuracy: 0.827\n",
      "epoch 27, iter: 287, loss: 1.210, accuracy: 0.831\n",
      "epoch 28, iter: 287, loss: 1.248, accuracy: 0.834\n",
      "epoch 29, iter: 287, loss: 1.251, accuracy: 0.835\n",
      "epoch 30, iter: 287, loss: 1.343, accuracy: 0.830\n",
      "epoch 31, iter: 287, loss: 1.287, accuracy: 0.838\n",
      "epoch 32, iter: 287, loss: 1.423, accuracy: 0.828\n",
      "epoch 33, iter: 287, loss: 1.510, accuracy: 0.825\n",
      "epoch 34, iter: 287, loss: 1.441, accuracy: 0.832\n",
      "epoch 35, iter: 287, loss: 1.654, accuracy: 0.818\n",
      "epoch 36, iter: 287, loss: 1.623, accuracy: 0.824\n",
      "epoch 37, iter: 287, loss: 1.699, accuracy: 0.832\n",
      "epoch 38, iter: 287, loss: 1.707, accuracy: 0.835\n",
      "epoch 39, iter: 287, loss: 1.780, accuracy: 0.833\n",
      "epoch 40, iter: 287, loss: 1.721, accuracy: 0.842\n",
      "epoch 41, iter: 287, loss: 1.774, accuracy: 0.838\n",
      "epoch 42, iter: 287, loss: 1.896, accuracy: 0.835\n",
      "epoch 43, iter: 287, loss: 1.975, accuracy: 0.834\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "print('Filter size = {}, stride size = {}'.format(32,3))\n",
    "modified_model_dict = apply_classification_loss_modification(cnn_modification, [32,3])\n",
    "train_model(modified_model_dict, dataset_generators, epoch_n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter size = 32, stride size = 4\n",
      "epoch 1, iter: 287, loss: 1.397, accuracy: 0.556\n",
      "epoch 2, iter: 287, loss: 1.046, accuracy: 0.684\n",
      "epoch 3, iter: 287, loss: 0.899, accuracy: 0.733\n",
      "epoch 4, iter: 287, loss: 0.820, accuracy: 0.759\n",
      "epoch 5, iter: 287, loss: 0.752, accuracy: 0.782\n",
      "epoch 6, iter: 287, loss: 0.752, accuracy: 0.786\n",
      "epoch 7, iter: 287, loss: 0.718, accuracy: 0.794\n",
      "epoch 8, iter: 287, loss: 0.705, accuracy: 0.800\n",
      "epoch 9, iter: 287, loss: 0.733, accuracy: 0.797\n",
      "epoch 10, iter: 287, loss: 0.662, accuracy: 0.816\n",
      "epoch 11, iter: 287, loss: 0.663, accuracy: 0.819\n",
      "epoch 12, iter: 287, loss: 0.644, accuracy: 0.823\n",
      "epoch 13, iter: 287, loss: 0.659, accuracy: 0.826\n",
      "epoch 14, iter: 287, loss: 0.685, accuracy: 0.820\n",
      "epoch 15, iter: 287, loss: 0.702, accuracy: 0.821\n",
      "epoch 16, iter: 287, loss: 0.716, accuracy: 0.816\n",
      "epoch 17, iter: 287, loss: 0.695, accuracy: 0.820\n",
      "epoch 18, iter: 287, loss: 0.692, accuracy: 0.825\n",
      "epoch 19, iter: 287, loss: 0.739, accuracy: 0.820\n",
      "epoch 20, iter: 287, loss: 0.728, accuracy: 0.824\n",
      "epoch 21, iter: 287, loss: 0.730, accuracy: 0.826\n",
      "epoch 22, iter: 287, loss: 0.738, accuracy: 0.827\n",
      "epoch 23, iter: 287, loss: 0.777, accuracy: 0.818\n",
      "epoch 24, iter: 287, loss: 0.783, accuracy: 0.824\n",
      "epoch 25, iter: 287, loss: 0.820, accuracy: 0.812\n",
      "epoch 26, iter: 287, loss: 0.889, accuracy: 0.805\n",
      "epoch 27, iter: 287, loss: 0.861, accuracy: 0.821\n",
      "epoch 28, iter: 287, loss: 0.903, accuracy: 0.815\n",
      "epoch 29, iter: 287, loss: 0.920, accuracy: 0.815\n",
      "epoch 30, iter: 287, loss: 0.981, accuracy: 0.811\n",
      "epoch 31, iter: 287, loss: 0.938, accuracy: 0.821\n",
      "epoch 32, iter: 287, loss: 0.969, accuracy: 0.820\n",
      "epoch 33, iter: 287, loss: 0.939, accuracy: 0.822\n",
      "epoch 34, iter: 287, loss: 0.962, accuracy: 0.828\n",
      "epoch 35, iter: 287, loss: 0.985, accuracy: 0.832\n",
      "epoch 36, iter: 287, loss: 1.052, accuracy: 0.821\n",
      "epoch 37, iter: 287, loss: 1.050, accuracy: 0.827\n",
      "epoch 38, iter: 287, loss: 1.134, accuracy: 0.820\n",
      "epoch 39, iter: 287, loss: 1.115, accuracy: 0.823\n",
      "epoch 40, iter: 287, loss: 1.136, accuracy: 0.828\n",
      "epoch 41, iter: 287, loss: 1.116, accuracy: 0.829\n",
      "epoch 42, iter: 287, loss: 1.197, accuracy: 0.823\n",
      "epoch 43, iter: 287, loss: 1.211, accuracy: 0.820\n",
      "epoch 44, iter: 287, loss: 1.255, accuracy: 0.815\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "print('Filter size = {}, stride size = {}'.format(32,4))\n",
    "modified_model_dict = apply_classification_loss_modification(cnn_modification, [32,4])\n",
    "train_model(modified_model_dict, dataset_generators, epoch_n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter size = 32, stride size = 5\n",
      "epoch 1, iter: 287, loss: 1.440, accuracy: 0.545\n",
      "epoch 2, iter: 287, loss: 1.046, accuracy: 0.686\n",
      "epoch 3, iter: 287, loss: 0.871, accuracy: 0.742\n",
      "epoch 4, iter: 287, loss: 0.797, accuracy: 0.768\n",
      "epoch 5, iter: 287, loss: 0.797, accuracy: 0.769\n",
      "epoch 6, iter: 287, loss: 0.746, accuracy: 0.786\n",
      "epoch 7, iter: 287, loss: 0.747, accuracy: 0.785\n",
      "epoch 8, iter: 287, loss: 0.719, accuracy: 0.794\n",
      "epoch 9, iter: 287, loss: 0.704, accuracy: 0.802\n",
      "epoch 10, iter: 287, loss: 0.728, accuracy: 0.797\n",
      "epoch 11, iter: 287, loss: 0.737, accuracy: 0.795\n",
      "epoch 12, iter: 287, loss: 0.727, accuracy: 0.799\n",
      "epoch 13, iter: 287, loss: 0.716, accuracy: 0.800\n",
      "epoch 14, iter: 287, loss: 0.725, accuracy: 0.804\n",
      "epoch 15, iter: 287, loss: 0.742, accuracy: 0.804\n",
      "epoch 16, iter: 287, loss: 0.726, accuracy: 0.811\n",
      "epoch 17, iter: 287, loss: 0.720, accuracy: 0.817\n",
      "epoch 18, iter: 287, loss: 0.779, accuracy: 0.802\n",
      "epoch 19, iter: 287, loss: 0.803, accuracy: 0.799\n",
      "epoch 20, iter: 287, loss: 0.837, accuracy: 0.802\n",
      "epoch 21, iter: 287, loss: 0.883, accuracy: 0.784\n",
      "epoch 22, iter: 287, loss: 0.879, accuracy: 0.793\n",
      "epoch 23, iter: 287, loss: 0.950, accuracy: 0.783\n",
      "epoch 24, iter: 287, loss: 0.859, accuracy: 0.807\n",
      "epoch 25, iter: 287, loss: 0.856, accuracy: 0.811\n",
      "epoch 26, iter: 287, loss: 0.878, accuracy: 0.809\n",
      "epoch 27, iter: 287, loss: 0.858, accuracy: 0.815\n",
      "epoch 28, iter: 287, loss: 0.843, accuracy: 0.818\n",
      "epoch 29, iter: 287, loss: 0.884, accuracy: 0.817\n",
      "epoch 30, iter: 287, loss: 0.912, accuracy: 0.814\n",
      "epoch 31, iter: 287, loss: 0.896, accuracy: 0.819\n",
      "epoch 32, iter: 287, loss: 0.910, accuracy: 0.818\n",
      "epoch 33, iter: 287, loss: 0.979, accuracy: 0.814\n",
      "epoch 34, iter: 287, loss: 0.987, accuracy: 0.807\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "print('Filter size = {}, stride size = {}'.format(32,5))\n",
    "modified_model_dict = apply_classification_loss_modification(cnn_modification, [32,5])\n",
    "train_model(modified_model_dict, dataset_generators, epoch_n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter size = 28, stride size = 2\n",
      "epoch 1, iter: 287, loss: 2.237, accuracy: 0.197\n",
      "epoch 2, iter: 287, loss: 2.199, accuracy: 0.211\n",
      "epoch 3, iter: 287, loss: 2.062, accuracy: 0.278\n",
      "epoch 4, iter: 287, loss: 1.753, accuracy: 0.398\n",
      "epoch 5, iter: 287, loss: 0.809, accuracy: 0.765\n",
      "epoch 6, iter: 287, loss: 0.677, accuracy: 0.816\n",
      "epoch 7, iter: 287, loss: 0.785, accuracy: 0.802\n",
      "epoch 8, iter: 287, loss: 0.671, accuracy: 0.841\n",
      "epoch 9, iter: 287, loss: 0.685, accuracy: 0.846\n",
      "epoch 10, iter: 287, loss: 0.753, accuracy: 0.838\n",
      "epoch 11, iter: 287, loss: 0.793, accuracy: 0.841\n",
      "epoch 12, iter: 287, loss: 0.856, accuracy: 0.843\n",
      "epoch 13, iter: 287, loss: 0.946, accuracy: 0.843\n",
      "epoch 14, iter: 287, loss: 1.005, accuracy: 0.845\n",
      "epoch 15, iter: 287, loss: 1.061, accuracy: 0.845\n",
      "epoch 16, iter: 287, loss: 1.121, accuracy: 0.846\n",
      "epoch 17, iter: 287, loss: 1.202, accuracy: 0.840\n",
      "epoch 18, iter: 287, loss: 1.347, accuracy: 0.829\n",
      "epoch 19, iter: 287, loss: 1.472, accuracy: 0.840\n",
      "epoch 20, iter: 287, loss: 1.527, accuracy: 0.831\n",
      "epoch 21, iter: 287, loss: 1.444, accuracy: 0.850\n",
      "epoch 22, iter: 287, loss: 1.515, accuracy: 0.850\n",
      "epoch 23, iter: 287, loss: 1.513, accuracy: 0.850\n",
      "epoch 24, iter: 287, loss: 1.718, accuracy: 0.847\n",
      "epoch 25, iter: 287, loss: 1.610, accuracy: 0.855\n",
      "epoch 26, iter: 287, loss: 1.632, accuracy: 0.848\n",
      "epoch 27, iter: 287, loss: 1.654, accuracy: 0.850\n",
      "epoch 28, iter: 287, loss: 1.824, accuracy: 0.850\n",
      "epoch 29, iter: 287, loss: 1.860, accuracy: 0.851\n",
      "epoch 30, iter: 287, loss: 2.079, accuracy: 0.846\n",
      "epoch 31, iter: 287, loss: 1.912, accuracy: 0.853\n",
      "epoch 32, iter: 287, loss: 2.056, accuracy: 0.852\n",
      "epoch 33, iter: 287, loss: 2.176, accuracy: 0.852\n",
      "epoch 34, iter: 287, loss: 2.145, accuracy: 0.853\n",
      "epoch 35, iter: 287, loss: 2.435, accuracy: 0.851\n",
      "epoch 36, iter: 287, loss: 2.300, accuracy: 0.851\n",
      "epoch 37, iter: 287, loss: 2.554, accuracy: 0.850\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "print('Filter size = {}, stride size = {}'.format(28,2))\n",
    "modified_model_dict = apply_classification_loss_modification(cnn_modification, [28,2])\n",
    "train_model(modified_model_dict, dataset_generators, epoch_n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter size = 36, stride size = 2\n",
      "epoch 1, iter: 287, loss: 2.224, accuracy: 0.196\n",
      "epoch 2, iter: 287, loss: 2.224, accuracy: 0.196\n",
      "epoch 3, iter: 287, loss: 2.222, accuracy: 0.197\n",
      "epoch 4, iter: 287, loss: 2.222, accuracy: 0.196\n",
      "epoch 5, iter: 287, loss: 1.133, accuracy: 0.648\n",
      "epoch 6, iter: 287, loss: 0.964, accuracy: 0.710\n",
      "epoch 7, iter: 287, loss: 0.918, accuracy: 0.733\n",
      "epoch 8, iter: 287, loss: 0.896, accuracy: 0.741\n",
      "epoch 9, iter: 287, loss: 0.853, accuracy: 0.755\n",
      "epoch 10, iter: 287, loss: 0.891, accuracy: 0.746\n",
      "epoch 11, iter: 287, loss: 0.884, accuracy: 0.747\n",
      "epoch 12, iter: 287, loss: 0.879, accuracy: 0.751\n",
      "epoch 13, iter: 287, loss: 0.915, accuracy: 0.745\n",
      "epoch 14, iter: 287, loss: 0.866, accuracy: 0.766\n",
      "epoch 15, iter: 287, loss: 0.880, accuracy: 0.759\n",
      "epoch 16, iter: 287, loss: 0.903, accuracy: 0.760\n",
      "epoch 17, iter: 287, loss: 0.920, accuracy: 0.762\n",
      "epoch 18, iter: 287, loss: 0.984, accuracy: 0.753\n",
      "epoch 19, iter: 287, loss: 0.933, accuracy: 0.775\n",
      "epoch 20, iter: 287, loss: 0.929, accuracy: 0.780\n",
      "epoch 21, iter: 287, loss: 0.958, accuracy: 0.782\n",
      "epoch 22, iter: 287, loss: 1.009, accuracy: 0.782\n",
      "epoch 23, iter: 287, loss: 1.044, accuracy: 0.785\n",
      "epoch 24, iter: 287, loss: 1.109, accuracy: 0.773\n",
      "epoch 25, iter: 287, loss: 1.118, accuracy: 0.781\n",
      "epoch 26, iter: 287, loss: 1.196, accuracy: 0.788\n",
      "epoch 27, iter: 287, loss: 1.261, accuracy: 0.786\n",
      "epoch 28, iter: 287, loss: 1.425, accuracy: 0.773\n",
      "epoch 29, iter: 287, loss: 1.532, accuracy: 0.772\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "print('Filter size = {}, stride size = {}'.format(36,2))\n",
    "modified_model_dict = apply_classification_loss_modification(cnn_modification, [36,2])\n",
    "train_model(modified_model_dict, dataset_generators, epoch_n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter size = 40, stride size = 2\n",
      "epoch 1, iter: 287, loss: 1.205, accuracy: 0.620\n",
      "epoch 2, iter: 287, loss: 0.742, accuracy: 0.786\n",
      "epoch 3, iter: 287, loss: 0.698, accuracy: 0.804\n",
      "epoch 4, iter: 287, loss: 0.663, accuracy: 0.816\n",
      "epoch 5, iter: 287, loss: 0.659, accuracy: 0.822\n",
      "epoch 6, iter: 287, loss: 0.724, accuracy: 0.806\n",
      "epoch 7, iter: 287, loss: 0.707, accuracy: 0.828\n",
      "epoch 8, iter: 287, loss: 0.727, accuracy: 0.827\n",
      "epoch 9, iter: 287, loss: 0.760, accuracy: 0.821\n",
      "epoch 10, iter: 287, loss: 0.786, accuracy: 0.828\n",
      "epoch 11, iter: 287, loss: 0.866, accuracy: 0.820\n",
      "epoch 12, iter: 287, loss: 0.975, accuracy: 0.816\n",
      "epoch 13, iter: 287, loss: 0.945, accuracy: 0.821\n",
      "epoch 14, iter: 287, loss: 0.987, accuracy: 0.822\n",
      "epoch 15, iter: 287, loss: 1.047, accuracy: 0.825\n",
      "epoch 16, iter: 287, loss: 1.093, accuracy: 0.822\n",
      "epoch 17, iter: 287, loss: 1.122, accuracy: 0.825\n",
      "epoch 18, iter: 287, loss: 1.137, accuracy: 0.823\n",
      "epoch 19, iter: 287, loss: 1.187, accuracy: 0.826\n",
      "epoch 20, iter: 287, loss: 1.193, accuracy: 0.827\n",
      "epoch 21, iter: 287, loss: 1.315, accuracy: 0.822\n",
      "epoch 22, iter: 287, loss: 1.353, accuracy: 0.811\n",
      "epoch 23, iter: 287, loss: 1.351, accuracy: 0.816\n",
      "epoch 24, iter: 287, loss: 1.449, accuracy: 0.825\n",
      "epoch 25, iter: 287, loss: 1.458, accuracy: 0.823\n",
      "epoch 26, iter: 287, loss: 1.587, accuracy: 0.819\n",
      "epoch 27, iter: 287, loss: 1.663, accuracy: 0.823\n",
      "epoch 28, iter: 287, loss: 1.651, accuracy: 0.827\n",
      "epoch 29, iter: 287, loss: 1.740, accuracy: 0.824\n",
      "epoch 30, iter: 287, loss: 1.783, accuracy: 0.827\n",
      "epoch 31, iter: 287, loss: 1.729, accuracy: 0.827\n",
      "epoch 32, iter: 287, loss: 1.883, accuracy: 0.826\n",
      "epoch 33, iter: 287, loss: 1.953, accuracy: 0.832\n",
      "epoch 34, iter: 287, loss: 2.109, accuracy: 0.833\n",
      "epoch 35, iter: 287, loss: 2.271, accuracy: 0.821\n",
      "epoch 36, iter: 287, loss: 2.292, accuracy: 0.830\n",
      "epoch 37, iter: 287, loss: 2.688, accuracy: 0.825\n",
      "epoch 38, iter: 287, loss: 2.497, accuracy: 0.828\n",
      "epoch 39, iter: 287, loss: 2.727, accuracy: 0.828\n",
      "epoch 40, iter: 287, loss: 2.714, accuracy: 0.830\n",
      "epoch 41, iter: 287, loss: 2.834, accuracy: 0.824\n",
      "epoch 42, iter: 287, loss: 3.007, accuracy: 0.826\n",
      "epoch 43, iter: 287, loss: 2.857, accuracy: 0.827\n",
      "epoch 44, iter: 287, loss: 2.715, accuracy: 0.827\n",
      "epoch 45, iter: 287, loss: 2.875, accuracy: 0.825\n",
      "epoch 46, iter: 287, loss: 2.987, accuracy: 0.825\n",
      "epoch 47, iter: 287, loss: 3.072, accuracy: 0.831\n",
      "epoch 48, iter: 287, loss: 2.924, accuracy: 0.831\n",
      "epoch 49, iter: 287, loss: 3.113, accuracy: 0.832\n",
      "epoch 50, iter: 287, loss: 3.103, accuracy: 0.836\n",
      "epoch 51, iter: 287, loss: 3.333, accuracy: 0.835\n",
      "epoch 52, iter: 287, loss: 3.142, accuracy: 0.828\n",
      "epoch 53, iter: 287, loss: 3.535, accuracy: 0.832\n",
      "epoch 54, iter: 287, loss: 3.385, accuracy: 0.830\n",
      "epoch 55, iter: 287, loss: 3.305, accuracy: 0.830\n",
      "epoch 56, iter: 287, loss: 3.446, accuracy: 0.833\n",
      "epoch 57, iter: 287, loss: 3.446, accuracy: 0.830\n",
      "epoch 58, iter: 287, loss: 3.459, accuracy: 0.825\n",
      "epoch 59, iter: 287, loss: 3.579, accuracy: 0.828\n",
      "epoch 60, iter: 287, loss: 3.784, accuracy: 0.831\n",
      "epoch 61, iter: 287, loss: 3.528, accuracy: 0.834\n",
      "epoch 62, iter: 287, loss: 3.616, accuracy: 0.828\n",
      "epoch 63, iter: 287, loss: 3.778, accuracy: 0.827\n",
      "epoch 64, iter: 287, loss: 4.057, accuracy: 0.826\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "print('Filter size = {}, stride size = {}'.format(40,2))\n",
    "modified_model_dict = apply_classification_loss_modification(cnn_modification, [40,2])\n",
    "train_model(modified_model_dict, dataset_generators, epoch_n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 2: Saving and Reloading Model Weights\n",
    "(25 points)\n",
    "\n",
    "In this section you learn to save the weights of a trained model, and to load the weights of a saved model. This is really useful when we would like to load an already trained model in order to continue training or to fine-tune it. Often times we save “snapshots” of the trained model as training progresses in case the training is interrupted, or in case we would like to fall back to an earlier model, this is called snapshot saving.\n",
    "\n",
    "### Q2.1 Defining another network\n",
    "Define a network with a slightly different structure in `def cnn_expanded(x_)` below. `cnn_expanded` is an expanded version of `cnn_model`. \n",
    "It should have: \n",
    "- a different size of kernel for the last convolutional layer, \n",
    "- followed by one additional convolutional layer, and \n",
    "- followed by one additional pooling layer.\n",
    "\n",
    "The last fully-connected layer will stay the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define the new model (see cnn_map(x_) above for an example)\n",
    "def cnn_expanded(x_):\n",
    "    conv1 = tf.layers.conv2d(\n",
    "            inputs=x_,\n",
    "            filters=32,  # number of filters\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu,\n",
    "            name='conv1')\n",
    "    \n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, \n",
    "                                    pool_size=[2, 2], \n",
    "                                    strides=2)\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(\n",
    "            inputs=pool1,\n",
    "            filters=32, # number of filters\n",
    "            kernel_size=[6, 6],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu,\n",
    "            name='conv2')\n",
    "    \n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, \n",
    "                                    pool_size=[2, 2], \n",
    "                                    strides=2)\n",
    "    \n",
    "    conv3 = tf.layers.conv2d(\n",
    "            inputs=pool2,\n",
    "            filters=32, # number of filters\n",
    "            kernel_size=[7, 7],\n",
    "            padding=\"same\",\n",
    "            activation=tf.nn.relu,\n",
    "            name='conv3')\n",
    "    \n",
    "    pool3 = tf.layers.max_pooling2d(inputs=conv3, \n",
    "                                    pool_size=[2, 2], \n",
    "                                    strides=2)\n",
    "        \n",
    "    pool_flat = tf.contrib.layers.flatten(pool3, scope='pool2flat')\n",
    "    dense = tf.layers.dense(inputs=pool_flat, units=500, activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(inputs=dense, units=10)\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Q2.2 Saving and Loading Weights\n",
    "`new_train_model()` below has two additional parameters `save_model=False, load_model=False` than `train_model` defined previously. Modify `new_train_model()` such that it would \n",
    "- save weights after the training is complete if `save_model` is `True`, and\n",
    "- load weights on start-up before training if `load_model` is `True`.\n",
    "\n",
    "*Hint:*  `tf.train.Saver()`.\n",
    "\n",
    "Note: if you are unable to load weights into `cnn_expanded` network, use `cnn_map` in order to continue the assingment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def new_train_model(model_dict, dataset_generators, epoch_n, print_every=287,\n",
    "                    save_model=False, load_model=False):\n",
    "    \n",
    "    with model_dict['graph'].as_default(), tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "#         writer = tf.summary.FileWriter('./graph/', graph=tf.get_default_graph())\n",
    "        saver = tf.train.Saver(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'conv1'))\n",
    "            \n",
    "        if load_model:\n",
    "            saver.restore(sess, 'checkpoints/checkpoint.ckpt')\n",
    "            print('Model loaded')\n",
    "        \n",
    "        loss_tmp = 1000; acc_tmp = 0; patience_ct = 0\n",
    "        for epoch_i in range(epoch_n):\n",
    "            for iter_i, data_batch in enumerate(dataset_generators['train']):\n",
    "                train_feed_dict = dict(zip(model_dict['inputs'], data_batch))\n",
    "                sess.run(model_dict['train_op'], feed_dict=train_feed_dict)\n",
    "                \n",
    "                if iter_i % print_every == print_every-1:\n",
    "                    collect_arr = []\n",
    "                    for test_batch in dataset_generators['test']:\n",
    "                        test_feed_dict = dict(zip(model_dict['inputs'], test_batch))\n",
    "                        to_compute = [model_dict['loss'], model_dict['accuracy']]\n",
    "                        collect_arr.append(sess.run(to_compute, test_feed_dict))\n",
    "                    averages = np.mean(collect_arr, axis=0)\n",
    "                    fmt = (epoch_i+1, print_every, ) + tuple(averages)\n",
    "                    print('iteration {:d} {:d}\\t loss: {:.3f}, '\n",
    "                          'accuracy: {:.3f}'.format(*fmt))\n",
    "                    \n",
    "            # Early stopping with patience of 4 epoches\n",
    "            if averages[0] > loss_tmp and averages[1] < acc_tmp:\n",
    "                patience_ct += 1\n",
    "                if patience_ct == 3:\n",
    "                    print('Early stopping!'); break\n",
    "            else: patience_ct = 0\n",
    "            loss_tmp = averages[0]; acc_tmp = averages[1]\n",
    "        \n",
    "        if save_model:\n",
    "            saver.save(sess, 'checkpoints/checkpoint.ckpt')\n",
    "            print('Model saved')\n",
    "    \n",
    "\n",
    "def test_saving():\n",
    "    model_dict = apply_classification_loss(cnn_map)\n",
    "    new_train_model(model_dict, dataset_generators, epoch_n=100, print_every=287, save_model=True)\n",
    "    cnn_expanded_dict = apply_classification_loss(cnn_expanded)\n",
    "    new_train_model(cnn_expanded_dict, dataset_generators, epoch_n=10,print_every=287, load_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 287\t loss: 2.179, accuracy: 0.226\n",
      "iteration 2 287\t loss: 1.197, accuracy: 0.629\n",
      "iteration 3 287\t loss: 0.963, accuracy: 0.716\n",
      "iteration 4 287\t loss: 0.872, accuracy: 0.747\n",
      "iteration 5 287\t loss: 0.863, accuracy: 0.753\n",
      "iteration 6 287\t loss: 0.851, accuracy: 0.758\n",
      "iteration 7 287\t loss: 0.814, accuracy: 0.772\n",
      "iteration 8 287\t loss: 0.796, accuracy: 0.780\n",
      "iteration 9 287\t loss: 0.821, accuracy: 0.778\n",
      "iteration 10 287\t loss: 0.834, accuracy: 0.785\n",
      "iteration 11 287\t loss: 0.867, accuracy: 0.785\n",
      "iteration 12 287\t loss: 0.882, accuracy: 0.781\n",
      "iteration 13 287\t loss: 0.881, accuracy: 0.791\n",
      "iteration 14 287\t loss: 0.921, accuracy: 0.798\n",
      "iteration 15 287\t loss: 0.944, accuracy: 0.799\n",
      "iteration 16 287\t loss: 0.961, accuracy: 0.802\n",
      "iteration 17 287\t loss: 1.137, accuracy: 0.785\n",
      "iteration 18 287\t loss: 1.111, accuracy: 0.788\n",
      "iteration 19 287\t loss: 1.104, accuracy: 0.794\n",
      "iteration 20 287\t loss: 1.010, accuracy: 0.811\n",
      "iteration 21 287\t loss: 1.094, accuracy: 0.813\n",
      "iteration 22 287\t loss: 1.167, accuracy: 0.810\n",
      "iteration 23 287\t loss: 1.221, accuracy: 0.817\n",
      "iteration 24 287\t loss: 1.336, accuracy: 0.820\n",
      "iteration 25 287\t loss: 1.350, accuracy: 0.816\n",
      "iteration 26 287\t loss: 1.447, accuracy: 0.813\n",
      "iteration 27 287\t loss: 1.469, accuracy: 0.808\n",
      "iteration 28 287\t loss: 1.445, accuracy: 0.817\n",
      "iteration 29 287\t loss: 1.618, accuracy: 0.815\n",
      "iteration 30 287\t loss: 1.643, accuracy: 0.807\n",
      "iteration 31 287\t loss: 1.647, accuracy: 0.811\n",
      "iteration 32 287\t loss: 1.673, accuracy: 0.818\n",
      "iteration 33 287\t loss: 1.824, accuracy: 0.831\n",
      "iteration 34 287\t loss: 2.078, accuracy: 0.820\n",
      "iteration 35 287\t loss: 2.331, accuracy: 0.817\n",
      "iteration 36 287\t loss: 2.183, accuracy: 0.818\n",
      "iteration 37 287\t loss: 2.446, accuracy: 0.819\n",
      "iteration 38 287\t loss: 2.438, accuracy: 0.817\n",
      "iteration 39 287\t loss: 2.412, accuracy: 0.822\n",
      "iteration 40 287\t loss: 2.488, accuracy: 0.826\n",
      "iteration 41 287\t loss: 2.448, accuracy: 0.817\n",
      "iteration 42 287\t loss: 2.699, accuracy: 0.815\n",
      "iteration 43 287\t loss: 2.807, accuracy: 0.827\n",
      "iteration 44 287\t loss: 2.773, accuracy: 0.830\n",
      "iteration 45 287\t loss: 2.826, accuracy: 0.830\n",
      "iteration 46 287\t loss: 2.962, accuracy: 0.834\n",
      "iteration 47 287\t loss: 2.864, accuracy: 0.833\n",
      "iteration 48 287\t loss: 3.045, accuracy: 0.835\n",
      "iteration 49 287\t loss: 3.134, accuracy: 0.838\n",
      "iteration 50 287\t loss: 3.040, accuracy: 0.843\n",
      "iteration 51 287\t loss: 3.247, accuracy: 0.834\n",
      "iteration 52 287\t loss: 3.303, accuracy: 0.834\n",
      "iteration 53 287\t loss: 3.216, accuracy: 0.837\n",
      "iteration 54 287\t loss: 3.368, accuracy: 0.836\n",
      "iteration 55 287\t loss: 3.400, accuracy: 0.837\n",
      "iteration 56 287\t loss: 3.310, accuracy: 0.841\n",
      "iteration 57 287\t loss: 3.474, accuracy: 0.838\n",
      "iteration 58 287\t loss: 3.554, accuracy: 0.844\n",
      "iteration 59 287\t loss: 3.724, accuracy: 0.835\n",
      "iteration 60 287\t loss: 4.018, accuracy: 0.833\n",
      "iteration 61 287\t loss: 3.827, accuracy: 0.840\n",
      "iteration 62 287\t loss: 4.020, accuracy: 0.841\n",
      "iteration 63 287\t loss: 3.961, accuracy: 0.839\n",
      "iteration 64 287\t loss: 4.103, accuracy: 0.842\n",
      "iteration 65 287\t loss: 4.281, accuracy: 0.840\n",
      "iteration 66 287\t loss: 4.183, accuracy: 0.843\n",
      "iteration 67 287\t loss: 4.075, accuracy: 0.844\n",
      "iteration 68 287\t loss: 4.050, accuracy: 0.833\n",
      "iteration 69 287\t loss: 4.407, accuracy: 0.835\n",
      "iteration 70 287\t loss: 4.407, accuracy: 0.842\n",
      "iteration 71 287\t loss: 4.414, accuracy: 0.844\n",
      "iteration 72 287\t loss: 4.617, accuracy: 0.845\n",
      "iteration 73 287\t loss: 4.526, accuracy: 0.848\n",
      "iteration 74 287\t loss: 4.722, accuracy: 0.844\n",
      "iteration 75 287\t loss: 4.745, accuracy: 0.847\n",
      "iteration 76 287\t loss: 4.611, accuracy: 0.846\n",
      "iteration 77 287\t loss: 4.801, accuracy: 0.846\n",
      "iteration 78 287\t loss: 4.633, accuracy: 0.845\n",
      "iteration 79 287\t loss: 5.053, accuracy: 0.844\n",
      "iteration 80 287\t loss: 5.406, accuracy: 0.846\n",
      "iteration 81 287\t loss: 4.917, accuracy: 0.843\n",
      "iteration 82 287\t loss: 5.283, accuracy: 0.843\n",
      "iteration 83 287\t loss: 5.382, accuracy: 0.845\n",
      "iteration 84 287\t loss: 5.431, accuracy: 0.846\n",
      "iteration 85 287\t loss: 5.619, accuracy: 0.845\n",
      "iteration 86 287\t loss: 5.276, accuracy: 0.845\n",
      "iteration 87 287\t loss: 5.410, accuracy: 0.843\n",
      "iteration 88 287\t loss: 5.431, accuracy: 0.851\n",
      "iteration 89 287\t loss: 5.532, accuracy: 0.849\n",
      "iteration 90 287\t loss: 5.567, accuracy: 0.848\n",
      "iteration 91 287\t loss: 6.542, accuracy: 0.846\n",
      "iteration 92 287\t loss: 6.091, accuracy: 0.851\n",
      "iteration 93 287\t loss: 6.116, accuracy: 0.846\n",
      "iteration 94 287\t loss: 6.157, accuracy: 0.848\n",
      "iteration 95 287\t loss: 6.202, accuracy: 0.843\n",
      "iteration 96 287\t loss: 6.384, accuracy: 0.850\n",
      "iteration 97 287\t loss: 6.370, accuracy: 0.846\n",
      "iteration 98 287\t loss: 6.676, accuracy: 0.849\n",
      "iteration 99 287\t loss: 6.468, accuracy: 0.848\n",
      "iteration 100 287\t loss: 5.834, accuracy: 0.851\n",
      "Model saved\n",
      "Model loaded\n",
      "iteration 1 287\t loss: 0.926, accuracy: 0.721\n",
      "iteration 2 287\t loss: 0.644, accuracy: 0.819\n",
      "iteration 3 287\t loss: 0.540, accuracy: 0.848\n",
      "iteration 4 287\t loss: 0.502, accuracy: 0.861\n",
      "iteration 5 287\t loss: 0.492, accuracy: 0.865\n",
      "iteration 6 287\t loss: 0.490, accuracy: 0.866\n",
      "iteration 7 287\t loss: 0.495, accuracy: 0.874\n",
      "iteration 8 287\t loss: 0.514, accuracy: 0.872\n",
      "iteration 9 287\t loss: 0.504, accuracy: 0.878\n",
      "iteration 10 287\t loss: 0.529, accuracy: 0.874\n"
     ]
    }
   ],
   "source": [
    "# Early stopping with patience of 4 epoches\n",
    "test_saving()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 3: Fine-tuning a Pre-trained Network on CIFAR-10\n",
    "(20 points)\n",
    "\n",
    "[CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) is another popular benchmark for image classification.\n",
    "We provide you with modified verstion of the file cifar10.py from [https://github.com/Hvass-Labs/TensorFlow-Tutorials](https://github.com/Hvass-Labs/TensorFlow-Tutorials).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import read_cifar10 as cf10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We also provide a generator for the CIFAR-10 Dataset, yielding the next batch every time next is invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "@read_data.restartable\n",
    "def cifar10_dataset_generator(dataset_name, batch_size, restrict_size=1000):\n",
    "    assert dataset_name in ['train', 'test']\n",
    "    assert batch_size > 0 or batch_size == -1  # -1 for entire dataset\n",
    "    \n",
    "    X_all_unrestricted, y_all = (cf10.load_training_data() if dataset_name == 'train'\n",
    "                                 else cf10.load_test_data())\n",
    "    \n",
    "    actual_restrict_size = restrict_size if dataset_name == 'train' else int(1e10)\n",
    "    X_all = X_all_unrestricted[:actual_restrict_size]\n",
    "    data_len = X_all.shape[0]\n",
    "    batch_size = batch_size if batch_size > 0 else data_len\n",
    "    \n",
    "    X_all_padded = np.concatenate([X_all, X_all[:batch_size]], axis=0)\n",
    "    y_all_padded = np.concatenate([y_all, y_all[:batch_size]], axis=0)\n",
    "    \n",
    "    for slice_i in range(math.ceil(data_len / batch_size)):\n",
    "        idx = slice_i * batch_size\n",
    "        X_batch = X_all_padded[idx:idx + batch_size]*255\n",
    "        y_batch = np.ravel(y_all_padded[idx:idx + batch_size])\n",
    "        yield X_batch.astype(np.uint8), y_batch.astype(np.uint8)\n",
    "\n",
    "cifar10_dataset_generators = {\n",
    "    'train': cifar10_dataset_generator('train', 1000),\n",
    "    'test': cifar10_dataset_generator('test', -1)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Q3.1 Fine-tuning\n",
    "Let's fine-tune SVHN net on **1000 examples** from CIFAR-10. \n",
    "Compare test accuracies of the following scenarios: \n",
    "  - Training `cnn_map` from scratch on the 1000 CIFAR-10 examples\n",
    "  - Fine-tuning SVHN net (`cnn_map` trained on SVHN dataset) on 1000 exampes from CIFAR-10. Use `new_train_model()` defined above to load SVHN net weights, but train on the CIFAR-10 examples.\n",
    "  \n",
    "**Important:** please do not change the `restrict_size=1000` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 1\t loss: 41.097, accuracy: 0.101\n",
      "iteration 2 1\t loss: 31.596, accuracy: 0.105\n",
      "iteration 3 1\t loss: 22.548, accuracy: 0.122\n",
      "iteration 4 1\t loss: 13.890, accuracy: 0.102\n",
      "iteration 5 1\t loss: 7.505, accuracy: 0.101\n",
      "iteration 6 1\t loss: 5.358, accuracy: 0.100\n",
      "iteration 7 1\t loss: 4.123, accuracy: 0.108\n",
      "iteration 8 1\t loss: 3.340, accuracy: 0.104\n",
      "iteration 9 1\t loss: 2.878, accuracy: 0.101\n",
      "iteration 10 1\t loss: 2.561, accuracy: 0.105\n",
      "iteration 11 1\t loss: 2.386, accuracy: 0.116\n",
      "iteration 12 1\t loss: 2.327, accuracy: 0.123\n",
      "iteration 13 1\t loss: 2.306, accuracy: 0.125\n",
      "iteration 14 1\t loss: 2.288, accuracy: 0.133\n",
      "iteration 15 1\t loss: 2.281, accuracy: 0.139\n",
      "iteration 16 1\t loss: 2.278, accuracy: 0.142\n",
      "iteration 17 1\t loss: 2.265, accuracy: 0.146\n",
      "iteration 18 1\t loss: 2.251, accuracy: 0.152\n",
      "iteration 19 1\t loss: 2.239, accuracy: 0.161\n",
      "iteration 20 1\t loss: 2.228, accuracy: 0.165\n",
      "iteration 21 1\t loss: 2.208, accuracy: 0.180\n",
      "iteration 22 1\t loss: 2.186, accuracy: 0.193\n",
      "iteration 23 1\t loss: 2.171, accuracy: 0.196\n",
      "iteration 24 1\t loss: 2.145, accuracy: 0.205\n",
      "iteration 25 1\t loss: 2.124, accuracy: 0.224\n",
      "iteration 26 1\t loss: 2.106, accuracy: 0.227\n",
      "iteration 27 1\t loss: 2.090, accuracy: 0.237\n",
      "iteration 28 1\t loss: 2.067, accuracy: 0.245\n",
      "iteration 29 1\t loss: 2.049, accuracy: 0.262\n",
      "iteration 30 1\t loss: 2.043, accuracy: 0.270\n",
      "iteration 31 1\t loss: 2.124, accuracy: 0.257\n",
      "iteration 32 1\t loss: 2.155, accuracy: 0.248\n",
      "iteration 33 1\t loss: 2.091, accuracy: 0.250\n",
      "iteration 34 1\t loss: 2.055, accuracy: 0.267\n",
      "iteration 35 1\t loss: 2.127, accuracy: 0.265\n",
      "iteration 36 1\t loss: 2.007, accuracy: 0.289\n",
      "iteration 37 1\t loss: 2.031, accuracy: 0.273\n",
      "iteration 38 1\t loss: 1.991, accuracy: 0.281\n",
      "iteration 39 1\t loss: 1.988, accuracy: 0.291\n",
      "iteration 40 1\t loss: 2.041, accuracy: 0.284\n",
      "iteration 41 1\t loss: 1.963, accuracy: 0.298\n",
      "iteration 42 1\t loss: 1.991, accuracy: 0.287\n",
      "iteration 43 1\t loss: 1.976, accuracy: 0.290\n",
      "iteration 44 1\t loss: 1.977, accuracy: 0.297\n",
      "iteration 45 1\t loss: 1.999, accuracy: 0.301\n",
      "iteration 46 1\t loss: 1.948, accuracy: 0.310\n",
      "iteration 47 1\t loss: 1.953, accuracy: 0.308\n",
      "iteration 48 1\t loss: 1.937, accuracy: 0.318\n",
      "iteration 49 1\t loss: 1.980, accuracy: 0.312\n",
      "iteration 50 1\t loss: 1.940, accuracy: 0.317\n",
      "iteration 51 1\t loss: 1.942, accuracy: 0.311\n",
      "iteration 52 1\t loss: 1.932, accuracy: 0.325\n",
      "iteration 53 1\t loss: 1.964, accuracy: 0.326\n",
      "iteration 54 1\t loss: 1.940, accuracy: 0.330\n",
      "iteration 55 1\t loss: 1.943, accuracy: 0.324\n",
      "iteration 56 1\t loss: 1.946, accuracy: 0.335\n",
      "iteration 57 1\t loss: 1.940, accuracy: 0.337\n",
      "iteration 58 1\t loss: 1.938, accuracy: 0.334\n",
      "iteration 59 1\t loss: 1.935, accuracy: 0.339\n",
      "iteration 60 1\t loss: 1.965, accuracy: 0.339\n",
      "iteration 61 1\t loss: 1.956, accuracy: 0.339\n",
      "iteration 62 1\t loss: 1.955, accuracy: 0.342\n",
      "iteration 63 1\t loss: 1.980, accuracy: 0.348\n",
      "iteration 64 1\t loss: 1.974, accuracy: 0.346\n",
      "iteration 65 1\t loss: 1.994, accuracy: 0.347\n",
      "iteration 66 1\t loss: 2.042, accuracy: 0.339\n",
      "iteration 67 1\t loss: 2.177, accuracy: 0.327\n",
      "iteration 68 1\t loss: 2.189, accuracy: 0.307\n",
      "Early stopping!\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "## train Cifar-10 from scratch and save first conv layer's weight\n",
    "cnn_expanded_dict = apply_classification_loss(cnn_map)\n",
    "new_train_model(cnn_expanded_dict, cifar10_dataset_generators, epoch_n=100, print_every=1, save_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "iteration 1 287\t loss: 0.664, accuracy: 0.810\n",
      "iteration 2 287\t loss: 0.538, accuracy: 0.851\n",
      "iteration 3 287\t loss: 0.494, accuracy: 0.864\n",
      "iteration 4 287\t loss: 0.475, accuracy: 0.870\n",
      "iteration 5 287\t loss: 0.468, accuracy: 0.879\n",
      "iteration 6 287\t loss: 0.483, accuracy: 0.878\n",
      "iteration 7 287\t loss: 0.529, accuracy: 0.874\n",
      "iteration 8 287\t loss: 0.515, accuracy: 0.878\n",
      "iteration 9 287\t loss: 0.521, accuracy: 0.885\n",
      "iteration 10 287\t loss: 0.596, accuracy: 0.870\n",
      "iteration 11 287\t loss: 0.631, accuracy: 0.873\n",
      "iteration 12 287\t loss: 0.642, accuracy: 0.870\n",
      "iteration 13 287\t loss: 0.669, accuracy: 0.871\n",
      "iteration 14 287\t loss: 0.694, accuracy: 0.876\n",
      "iteration 15 287\t loss: 0.686, accuracy: 0.883\n",
      "iteration 16 287\t loss: 0.727, accuracy: 0.877\n",
      "iteration 17 287\t loss: 0.820, accuracy: 0.879\n",
      "iteration 18 287\t loss: 0.770, accuracy: 0.886\n",
      "iteration 19 287\t loss: 0.787, accuracy: 0.886\n",
      "iteration 20 287\t loss: 0.888, accuracy: 0.881\n",
      "iteration 21 287\t loss: 0.863, accuracy: 0.883\n",
      "iteration 22 287\t loss: 0.946, accuracy: 0.877\n",
      "iteration 23 287\t loss: 1.093, accuracy: 0.867\n",
      "iteration 24 287\t loss: 1.041, accuracy: 0.876\n",
      "iteration 25 287\t loss: 1.142, accuracy: 0.873\n",
      "iteration 26 287\t loss: 1.151, accuracy: 0.875\n",
      "iteration 27 287\t loss: 1.147, accuracy: 0.882\n",
      "iteration 28 287\t loss: 1.229, accuracy: 0.875\n",
      "iteration 29 287\t loss: 1.356, accuracy: 0.870\n",
      "iteration 30 287\t loss: 1.374, accuracy: 0.868\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "# fine-tuning SVHN Net using Cifar-10 weights saved above\n",
    "new_train_model(cnn_expanded_dict, dataset_generators, epoch_n=100, print_every=287, load_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "iteration 1 1\t loss: 14.328, accuracy: 0.115\n",
      "iteration 2 1\t loss: 10.007, accuracy: 0.115\n",
      "iteration 3 1\t loss: 7.000, accuracy: 0.117\n",
      "iteration 4 1\t loss: 4.704, accuracy: 0.157\n",
      "iteration 5 1\t loss: 3.532, accuracy: 0.147\n",
      "iteration 6 1\t loss: 2.832, accuracy: 0.144\n",
      "iteration 7 1\t loss: 2.485, accuracy: 0.151\n",
      "iteration 8 1\t loss: 2.349, accuracy: 0.149\n",
      "iteration 9 1\t loss: 2.305, accuracy: 0.135\n",
      "iteration 10 1\t loss: 2.290, accuracy: 0.127\n",
      "iteration 11 1\t loss: 2.278, accuracy: 0.128\n",
      "iteration 12 1\t loss: 2.267, accuracy: 0.128\n",
      "iteration 13 1\t loss: 2.255, accuracy: 0.132\n",
      "iteration 14 1\t loss: 2.243, accuracy: 0.137\n",
      "iteration 15 1\t loss: 2.228, accuracy: 0.148\n",
      "iteration 16 1\t loss: 2.213, accuracy: 0.160\n",
      "iteration 17 1\t loss: 2.198, accuracy: 0.171\n",
      "iteration 18 1\t loss: 2.176, accuracy: 0.195\n",
      "iteration 19 1\t loss: 2.149, accuracy: 0.221\n",
      "iteration 20 1\t loss: 2.120, accuracy: 0.235\n",
      "iteration 21 1\t loss: 2.089, accuracy: 0.249\n",
      "iteration 22 1\t loss: 2.056, accuracy: 0.263\n",
      "iteration 23 1\t loss: 2.025, accuracy: 0.267\n",
      "iteration 24 1\t loss: 2.004, accuracy: 0.274\n",
      "iteration 25 1\t loss: 2.001, accuracy: 0.275\n",
      "iteration 26 1\t loss: 2.024, accuracy: 0.281\n",
      "iteration 27 1\t loss: 2.049, accuracy: 0.278\n",
      "iteration 28 1\t loss: 1.957, accuracy: 0.301\n",
      "iteration 29 1\t loss: 1.999, accuracy: 0.294\n",
      "iteration 30 1\t loss: 1.927, accuracy: 0.320\n",
      "iteration 31 1\t loss: 1.943, accuracy: 0.317\n",
      "iteration 32 1\t loss: 1.909, accuracy: 0.325\n",
      "iteration 33 1\t loss: 1.904, accuracy: 0.332\n",
      "iteration 34 1\t loss: 1.910, accuracy: 0.336\n",
      "iteration 35 1\t loss: 1.892, accuracy: 0.335\n",
      "iteration 36 1\t loss: 1.906, accuracy: 0.333\n",
      "iteration 37 1\t loss: 1.890, accuracy: 0.344\n",
      "iteration 38 1\t loss: 1.905, accuracy: 0.348\n",
      "iteration 39 1\t loss: 1.894, accuracy: 0.349\n",
      "iteration 40 1\t loss: 1.917, accuracy: 0.343\n",
      "iteration 41 1\t loss: 1.908, accuracy: 0.351\n",
      "iteration 42 1\t loss: 1.932, accuracy: 0.357\n",
      "iteration 43 1\t loss: 1.929, accuracy: 0.357\n",
      "iteration 44 1\t loss: 1.918, accuracy: 0.365\n",
      "iteration 45 1\t loss: 1.934, accuracy: 0.368\n",
      "iteration 46 1\t loss: 1.954, accuracy: 0.368\n",
      "iteration 47 1\t loss: 1.976, accuracy: 0.362\n",
      "iteration 48 1\t loss: 1.992, accuracy: 0.368\n",
      "iteration 49 1\t loss: 2.034, accuracy: 0.366\n",
      "iteration 50 1\t loss: 2.038, accuracy: 0.368\n",
      "iteration 51 1\t loss: 2.028, accuracy: 0.376\n",
      "iteration 52 1\t loss: 2.087, accuracy: 0.373\n",
      "iteration 53 1\t loss: 2.112, accuracy: 0.373\n",
      "iteration 54 1\t loss: 2.125, accuracy: 0.375\n",
      "iteration 55 1\t loss: 2.188, accuracy: 0.376\n",
      "iteration 56 1\t loss: 2.167, accuracy: 0.375\n",
      "iteration 57 1\t loss: 2.232, accuracy: 0.370\n",
      "iteration 58 1\t loss: 2.250, accuracy: 0.380\n",
      "iteration 59 1\t loss: 2.300, accuracy: 0.378\n",
      "iteration 60 1\t loss: 2.320, accuracy: 0.377\n",
      "iteration 61 1\t loss: 2.372, accuracy: 0.375\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "# fine-tuning Cifar-10 using Cifar-10 weights saved above\n",
    "new_train_model(cnn_expanded_dict, cifar10_dataset_generators, epoch_n=100, print_every=1, load_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "iteration 1 1\t loss: 45.584, accuracy: 0.127\n",
      "iteration 2 1\t loss: 45.845, accuracy: 0.107\n",
      "iteration 3 1\t loss: 44.642, accuracy: 0.111\n",
      "iteration 4 1\t loss: 31.459, accuracy: 0.113\n",
      "iteration 5 1\t loss: 19.414, accuracy: 0.132\n",
      "iteration 6 1\t loss: 11.665, accuracy: 0.130\n",
      "iteration 7 1\t loss: 6.654, accuracy: 0.126\n",
      "iteration 8 1\t loss: 4.165, accuracy: 0.120\n",
      "iteration 9 1\t loss: 3.077, accuracy: 0.114\n",
      "iteration 10 1\t loss: 2.629, accuracy: 0.111\n",
      "iteration 11 1\t loss: 2.448, accuracy: 0.107\n",
      "iteration 12 1\t loss: 2.374, accuracy: 0.101\n",
      "iteration 13 1\t loss: 2.341, accuracy: 0.101\n",
      "iteration 14 1\t loss: 2.326, accuracy: 0.103\n",
      "iteration 15 1\t loss: 2.318, accuracy: 0.105\n",
      "iteration 16 1\t loss: 2.313, accuracy: 0.107\n",
      "iteration 17 1\t loss: 2.309, accuracy: 0.108\n",
      "iteration 18 1\t loss: 2.307, accuracy: 0.112\n",
      "iteration 19 1\t loss: 2.306, accuracy: 0.109\n",
      "iteration 20 1\t loss: 2.304, accuracy: 0.108\n",
      "iteration 21 1\t loss: 2.303, accuracy: 0.109\n",
      "iteration 22 1\t loss: 2.302, accuracy: 0.111\n",
      "iteration 23 1\t loss: 2.301, accuracy: 0.111\n",
      "iteration 24 1\t loss: 2.301, accuracy: 0.111\n",
      "iteration 25 1\t loss: 2.300, accuracy: 0.112\n",
      "iteration 26 1\t loss: 2.299, accuracy: 0.113\n",
      "iteration 27 1\t loss: 2.298, accuracy: 0.113\n",
      "iteration 28 1\t loss: 2.297, accuracy: 0.115\n",
      "iteration 29 1\t loss: 2.296, accuracy: 0.116\n",
      "iteration 30 1\t loss: 2.295, accuracy: 0.105\n",
      "iteration 31 1\t loss: 2.293, accuracy: 0.107\n",
      "iteration 32 1\t loss: 2.291, accuracy: 0.108\n",
      "iteration 33 1\t loss: 2.289, accuracy: 0.106\n",
      "iteration 34 1\t loss: 2.287, accuracy: 0.106\n",
      "iteration 35 1\t loss: 2.285, accuracy: 0.107\n",
      "iteration 36 1\t loss: 2.283, accuracy: 0.108\n",
      "iteration 37 1\t loss: 2.280, accuracy: 0.107\n",
      "iteration 38 1\t loss: 2.279, accuracy: 0.108\n",
      "iteration 39 1\t loss: 2.278, accuracy: 0.109\n",
      "iteration 40 1\t loss: 2.278, accuracy: 0.109\n",
      "iteration 41 1\t loss: 2.279, accuracy: 0.110\n",
      "iteration 42 1\t loss: 2.279, accuracy: 0.110\n",
      "iteration 43 1\t loss: 2.278, accuracy: 0.111\n",
      "iteration 44 1\t loss: 2.274, accuracy: 0.113\n",
      "iteration 45 1\t loss: 2.269, accuracy: 0.116\n",
      "iteration 46 1\t loss: 2.265, accuracy: 0.122\n",
      "iteration 47 1\t loss: 2.261, accuracy: 0.134\n",
      "iteration 48 1\t loss: 2.257, accuracy: 0.136\n",
      "iteration 49 1\t loss: 2.254, accuracy: 0.139\n",
      "iteration 50 1\t loss: 2.251, accuracy: 0.141\n",
      "iteration 51 1\t loss: 2.248, accuracy: 0.145\n",
      "iteration 52 1\t loss: 2.245, accuracy: 0.151\n",
      "iteration 53 1\t loss: 2.243, accuracy: 0.154\n",
      "iteration 54 1\t loss: 2.240, accuracy: 0.156\n",
      "iteration 55 1\t loss: 2.238, accuracy: 0.157\n",
      "iteration 56 1\t loss: 2.236, accuracy: 0.157\n",
      "iteration 57 1\t loss: 2.233, accuracy: 0.161\n",
      "iteration 58 1\t loss: 2.227, accuracy: 0.161\n",
      "iteration 59 1\t loss: 2.221, accuracy: 0.165\n",
      "iteration 60 1\t loss: 2.216, accuracy: 0.167\n",
      "iteration 61 1\t loss: 2.213, accuracy: 0.167\n",
      "iteration 62 1\t loss: 2.211, accuracy: 0.169\n",
      "iteration 63 1\t loss: 2.207, accuracy: 0.171\n",
      "iteration 64 1\t loss: 2.203, accuracy: 0.173\n",
      "iteration 65 1\t loss: 2.202, accuracy: 0.175\n",
      "iteration 66 1\t loss: 2.201, accuracy: 0.175\n",
      "iteration 67 1\t loss: 2.201, accuracy: 0.177\n",
      "iteration 68 1\t loss: 2.195, accuracy: 0.184\n",
      "iteration 69 1\t loss: 2.190, accuracy: 0.189\n",
      "iteration 70 1\t loss: 2.188, accuracy: 0.187\n",
      "iteration 71 1\t loss: 2.187, accuracy: 0.190\n",
      "iteration 72 1\t loss: 2.183, accuracy: 0.193\n",
      "iteration 73 1\t loss: 2.181, accuracy: 0.194\n",
      "iteration 74 1\t loss: 2.181, accuracy: 0.195\n",
      "iteration 75 1\t loss: 2.182, accuracy: 0.195\n",
      "iteration 76 1\t loss: 2.179, accuracy: 0.197\n",
      "iteration 77 1\t loss: 2.176, accuracy: 0.200\n",
      "iteration 78 1\t loss: 2.176, accuracy: 0.202\n",
      "iteration 79 1\t loss: 2.177, accuracy: 0.205\n",
      "iteration 80 1\t loss: 2.174, accuracy: 0.207\n",
      "iteration 81 1\t loss: 2.176, accuracy: 0.210\n",
      "iteration 82 1\t loss: 2.181, accuracy: 0.214\n",
      "iteration 83 1\t loss: 2.180, accuracy: 0.217\n",
      "iteration 84 1\t loss: 2.178, accuracy: 0.218\n",
      "iteration 85 1\t loss: 2.184, accuracy: 0.223\n",
      "iteration 86 1\t loss: 2.187, accuracy: 0.225\n",
      "iteration 87 1\t loss: 2.189, accuracy: 0.225\n",
      "iteration 88 1\t loss: 2.196, accuracy: 0.230\n",
      "iteration 89 1\t loss: 2.201, accuracy: 0.230\n",
      "iteration 90 1\t loss: 2.201, accuracy: 0.228\n",
      "iteration 91 1\t loss: 2.205, accuracy: 0.230\n",
      "iteration 92 1\t loss: 2.204, accuracy: 0.231\n",
      "iteration 93 1\t loss: 2.203, accuracy: 0.235\n",
      "iteration 94 1\t loss: 2.212, accuracy: 0.237\n",
      "iteration 95 1\t loss: 2.220, accuracy: 0.236\n",
      "iteration 96 1\t loss: 2.220, accuracy: 0.237\n",
      "iteration 97 1\t loss: 2.222, accuracy: 0.241\n",
      "iteration 98 1\t loss: 2.228, accuracy: 0.240\n",
      "iteration 99 1\t loss: 2.223, accuracy: 0.239\n",
      "iteration 100 1\t loss: 2.240, accuracy: 0.241\n"
     ]
    }
   ],
   "source": [
    "# fine-tuning Cifar-10 using SVHN's pretrain weights from Q2\n",
    "new_train_model(cnn_expanded_dict, cifar10_dataset_generators, epoch_n=100, print_every=1, load_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Pretraining on | Fine tunning on | Accuracy |\n",
    "|----------------|----------------|----------|\n",
    "| SVHN | SVHN | 0.874 |\n",
    "| Cifar-10 | SVHN | 0.868 |\n",
    "| Cifar-10 | Cifar-10 | 0.375 |\n",
    "| SVHN | Cifar-10 | 0.241 |\n",
    "\n",
    "__Since SVHN and Cifar-10 are very different image set, fine tunning either of them using the pretrained weight from the other actually lower the accuaracy. However, using Cifar-10 pretrained first layer to initialize Cifer-10 network helps narrowing down the search direction of the network, therefore, the much better accuracy is achieved.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 4: TensorBoard\n",
    "(30 points)\n",
    "\n",
    "[TensorBoard](https://www.tensorflow.org/get_started/summaries_and_tensorboard) is a very helpful tool for visualization of neural networks. \n",
    "\n",
    "### Q4.1 Plotting\n",
    "Present at least one visualization for each of the following:\n",
    "  - Filters\n",
    "  - Loss\n",
    "  - Accuracy\n",
    "\n",
    "Modify code you have wrote above to also have summary writers. To  run tensorboard, the command is\n",
    "\n",
    "tensorboard --logdir=./graph --port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' \n",
    "Thsi code is from: \n",
    "https://gist.github.com/kukuruza/03731dc494603ceab0c5\n",
    "'''\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "def put_kernels_on_grid (kernel, pad = 2):\n",
    "\n",
    "    '''Visualize conv. features as an image (mostly for the 1st layer).\n",
    "    Place kernel into a grid, with some paddings between adjacent filters.\n",
    "    Args:\n",
    "      kernel:            tensor of shape [Y, X, NumChannels, NumKernels]\n",
    "      (grid_Y, grid_X):  shape of the grid. Require: NumKernels == grid_Y * grid_X\n",
    "                           User is responsible of how to break into two multiples.\n",
    "      pad:               number of black pixels around each filter (between them)\n",
    "    Return:\n",
    "      Tensor of shape [(Y+2*pad)*grid_Y, (X+2*pad)*grid_X, NumChannels, 1].\n",
    "    '''\n",
    "    # get shape of the grid. NumKernels == grid_Y * grid_X\n",
    "    def factorization(n):\n",
    "        for i in range(int(sqrt(float(n))), 0, -1):\n",
    "            if n % i == 0:\n",
    "                if i == 1: print('Who would enter a prime number of filters')\n",
    "                return (i, int(n / i))\n",
    "    (grid_Y, grid_X) = factorization (kernel.get_shape()[3].value)\n",
    "    print ('grid: %d = (%d, %d)' % (kernel.get_shape()[3].value, grid_Y, grid_X))\n",
    "\n",
    "    x_min = tf.reduce_min(kernel)\n",
    "    x_max = tf.reduce_max(kernel)\n",
    "\n",
    "    kernel1 = (kernel - x_min) / (x_max - x_min)\n",
    "\n",
    "    # pad X and Y\n",
    "    x1 = tf.pad(kernel1, tf.constant( [[pad,pad],[pad, pad],[0,0],[0,0]] ), mode = 'CONSTANT')\n",
    "\n",
    "    # X and Y dimensions, w.r.t. padding\n",
    "    Y = kernel1.get_shape()[0] + 2 * pad\n",
    "    X = kernel1.get_shape()[1] + 2 * pad\n",
    "\n",
    "    channels = kernel1.get_shape()[2]\n",
    "\n",
    "    # put NumKernels to the 1st dimension\n",
    "    x2 = tf.transpose(x1, (3, 0, 1, 2))\n",
    "    # organize grid on Y axis\n",
    "    x3 = tf.reshape(x2, tf.stack([grid_X, Y * grid_Y, X, channels]))\n",
    "\n",
    "    # switch X and Y axes\n",
    "    x4 = tf.transpose(x3, (0, 2, 1, 3))\n",
    "    # organize grid on X axis\n",
    "    x5 = tf.reshape(x4, tf.stack([1, X * grid_X, Y * grid_Y, channels]))\n",
    "\n",
    "    # back to normal order (not combining with the next step for clarity)\n",
    "    x6 = tf.transpose(x5, (2, 1, 3, 0))\n",
    "\n",
    "    # to tf.image_summary order [batch_size, height, width, channels],\n",
    "    #   where in this case batch_size == 1\n",
    "    x7 = tf.transpose(x6, (3, 0, 1, 2))\n",
    "\n",
    "    # scaling to [0, 255] is not necessary for tensorboard\n",
    "    return x7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def visualize(dataset_generators, epoch_n, print_every=287):\n",
    "    model_dict = apply_classification_loss(cnn_map)\n",
    "    \n",
    "    with model_dict['graph'].as_default(), tf.Session() as sess:\n",
    "        # Define sumaries\n",
    "        tf.summary.scalar('accuracy', model_dict['accuracy'])\n",
    "        tf.summary.scalar('loss', model_dict['loss'])\n",
    "        \n",
    "        kernel = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'conv1')[0]\n",
    "        grid = put_kernels_on_grid (kernel)\n",
    "        tf.summary.image('conv1/features', grid, max_outputs=1)\n",
    "    \n",
    "        tf.summary.histogram('filter_weight', tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'conv1')[0])\n",
    "        tf.summary.histogram('filter_bias', tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'conv1')[1])\n",
    "        merged = tf.summary.merge_all()\n",
    "        # Initial writer\n",
    "        train_writer = tf.summary.FileWriter('./graph' + '/train',\n",
    "                                      sess.graph)\n",
    "        test_writer = tf.summary.FileWriter('./graph' + '/test')\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        loss_tmp = 1000; acc_tmp = 0.0\n",
    "        for epoch_i in range(epoch_n):\n",
    "            for iter_i, data_batch in enumerate(dataset_generators['train']):\n",
    "                train_feed_dict = dict(zip(model_dict['inputs'], data_batch))\n",
    "                sess.run([model_dict['train_op'], ], feed_dict=train_feed_dict)\n",
    "                \n",
    "                # Vistualize training\n",
    "                summary_train = sess.run(merged, feed_dict=train_feed_dict)\n",
    "                train_writer.add_summary(summary_train, epoch_i) \n",
    "                \n",
    "                if iter_i % print_every == print_every-1:\n",
    "                    collect_arr = []\n",
    "                    for test_batch in dataset_generators['test']:\n",
    "                        test_feed_dict = dict(zip(model_dict['inputs'], test_batch))\n",
    "                        to_compute = [model_dict['loss'], model_dict['accuracy']]\n",
    "                        collect_arr.append(sess.run(to_compute, test_feed_dict)) \n",
    "                        \n",
    "                        # Vistualize testing\n",
    "                        summary_test = sess.run(merged, feed_dict=test_feed_dict)\n",
    "                        test_writer.add_summary(summary_test, epoch_i)\n",
    "                        \n",
    "                    averages = np.mean(collect_arr, axis=0)\n",
    "                    fmt = (epoch_i+1, print_every) + tuple(averages)\n",
    "                    print('epoch {:d}, iter: {:d}, loss: {:.3f}, '\n",
    "                          'accuracy: {:.3f}'.format(*fmt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid: 32 = (4, 8)\n",
      "epoch 1, iter: 287, loss: 0.880, accuracy: 0.741\n",
      "epoch 2, iter: 287, loss: 0.806, accuracy: 0.769\n",
      "epoch 3, iter: 287, loss: 0.750, accuracy: 0.792\n",
      "epoch 4, iter: 287, loss: 0.700, accuracy: 0.808\n",
      "epoch 5, iter: 287, loss: 0.748, accuracy: 0.798\n",
      "epoch 6, iter: 287, loss: 0.793, accuracy: 0.796\n",
      "epoch 7, iter: 287, loss: 0.815, accuracy: 0.801\n",
      "epoch 8, iter: 287, loss: 0.847, accuracy: 0.797\n",
      "epoch 9, iter: 287, loss: 0.903, accuracy: 0.783\n",
      "epoch 10, iter: 287, loss: 0.847, accuracy: 0.802\n"
     ]
    }
   ],
   "source": [
    "visualize(dataset_generators, epoch_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Loss and accuracy:__\n",
    "\n",
    "<img width='600px' src=\"https://raw.githubusercontent.com/GordonCai/BU-EC500K-Deep-Learning/master/Homework/hw4/Vistualization/vanila%20loss%20accuracy.jpeg\">\n",
    "\n",
    "__Graph:__\n",
    "\n",
    "<img width='800px' src=\"https://raw.githubusercontent.com/GordonCai/BU-EC500K-Deep-Learning/master/Homework/hw4/Vistualization/vanilla%20graph.jpeg\">\n",
    "\n",
    "__Conv1 image:__\n",
    "\n",
    "<img width='800px' src=\"https://raw.githubusercontent.com/GordonCai/BU-EC500K-Deep-Learning/master/Homework/hw4/Vistualization/vanilla%20image.jpeg\">\n",
    "\n",
    "__Weights and bias histogram:__\n",
    "\n",
    "<img width='400px' src=\"https://raw.githubusercontent.com/GordonCai/BU-EC500K-Deep-Learning/master/Homework/hw4/Vistualization/vanilla%20hist.jpeg\">\n",
    "\n",
    "__Weights and bias distribution:__\n",
    "\n",
    "<img width='400px' src=\"https://raw.githubusercontent.com/GordonCai/BU-EC500K-Deep-Learning/master/Homework/hw4/Vistualization/vanilla%20dist.jpeg\">\n",
    "\n",
    "\n",
    "__The result shows significant overfitting after only about 6 epoch. Weight change is not much in conv 1, but the bias is increasing.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Part 5: Bonus\n",
    "(20 points)\n",
    "\n",
    "__I tried both RESNet-like and VGGNet-like architecture, and use SVHN pretraining from question 2 to narrow down the search direction of the network, giving it a kick start.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 shallow non-bottle-necked ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width='400px' src=\"https://raw.githubusercontent.com/GordonCai/BU-EC500K-Deep-Learning/master/Homework/hw4/Vistualization/res%20graph%201.jpeg\">\n",
    "\n",
    "<img width='400px' src=\"https://raw.githubusercontent.com/GordonCai/BU-EC500K-Deep-Learning/master/Homework/hw4/Vistualization/res%20graph%202.jpeg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cnn_map_ResNet(x_, keep_prob, reg, res_layers, dense_layers):\n",
    "    conv1 = tf.layers.conv2d(\n",
    "            inputs=x_,\n",
    "            filters=32,\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            kernel_regularizer = tf.contrib.layers.l2_regularizer(reg),\n",
    "            activation=tf.nn.relu,\n",
    "            name='conv1')\n",
    "    \n",
    "    maxpool = tf.layers.max_pooling2d(inputs=conv1, \n",
    "                                    pool_size=[2, 2], \n",
    "                                    strides=2)\n",
    "    \n",
    "    # Block 1\n",
    "    with tf.variable_scope('block1'): \n",
    "        conv2 = tf.layers.conv2d(\n",
    "                inputs=maxpool,\n",
    "                filters=res_layers[0],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(reg),\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "        conv3 = tf.layers.conv2d(\n",
    "                inputs=conv2,\n",
    "                filters=res_layers[0],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(reg),\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "        conv4 = tf.layers.conv2d(\n",
    "                inputs=conv3,\n",
    "                filters=res_layers[0],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(reg),\n",
    "                activation=None)\n",
    "\n",
    "        combine1 = tf.nn.relu(tf.add(conv4, conv2))\n",
    "\n",
    "        conv5 = tf.layers.conv2d(\n",
    "                inputs=conv4,\n",
    "                filters=res_layers[0],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(reg),\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "        conv6 = tf.layers.conv2d(\n",
    "                inputs=conv5,\n",
    "                filters=res_layers[0],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(reg),\n",
    "                activation=None)\n",
    "\n",
    "        combine2 = tf.nn.relu(tf.add(conv6, combine1))\n",
    "    \n",
    "    # Block 2\n",
    "    with tf.variable_scope('block2'):\n",
    "        conv7 = tf.layers.conv2d(\n",
    "                inputs=combine2,\n",
    "                filters=res_layers[1],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(reg),\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "        conv8 = tf.layers.conv2d(\n",
    "                inputs=conv7,\n",
    "                filters=res_layers[1],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(reg),\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "        conv9 = tf.layers.conv2d(\n",
    "                inputs=conv8,\n",
    "                filters=res_layers[1],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(reg),\n",
    "                activation=None)\n",
    "\n",
    "        combine3 = tf.nn.relu(tf.add(conv7, conv9))\n",
    "\n",
    "        conv10 = tf.layers.conv2d(\n",
    "                inputs=combine3,\n",
    "                filters=res_layers[1],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(reg),\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "        conv11 = tf.layers.conv2d(\n",
    "                inputs=conv10,\n",
    "                filters=res_layers[1],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(reg),\n",
    "                activation=None)\n",
    "\n",
    "        combine4 = tf.nn.relu(tf.add(conv11, combine3))\n",
    "    \n",
    "    # Block 3\n",
    "    with tf.variable_scope('block3'):\n",
    "        conv12 = tf.layers.conv2d(\n",
    "                inputs=combine4,\n",
    "                filters=res_layers[2],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(reg),\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "        conv13 = tf.layers.conv2d(\n",
    "                inputs=conv12,\n",
    "                filters=res_layers[2],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(reg),\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "        conv14 = tf.layers.conv2d(\n",
    "                inputs=conv13,\n",
    "                filters=res_layers[2],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(reg),\n",
    "                activation=None)\n",
    "\n",
    "        combine5 = tf.nn.relu(tf.add(conv14, conv12))\n",
    "\n",
    "        conv15 = tf.layers.conv2d(\n",
    "                inputs=combine5,\n",
    "                filters=res_layers[2],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(reg),\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "        conv16 = tf.layers.conv2d(\n",
    "                inputs=conv15,\n",
    "                filters=res_layers[2],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(reg),\n",
    "                activation=None)\n",
    "\n",
    "        combine6 = tf.nn.relu(tf.add(conv16, combine5))\n",
    "\n",
    "        conv17 = tf.layers.conv2d(\n",
    "                inputs=combine6,\n",
    "                filters=res_layers[2],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(reg),\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "        conv18 = tf.layers.conv2d(\n",
    "                inputs=conv17,\n",
    "                filters=res_layers[2],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(reg),\n",
    "                activation=None)\n",
    "\n",
    "        combine7 = tf.nn.relu(tf.add(conv18, combine6))\n",
    "    \n",
    "    # Block 4\n",
    "    with tf.variable_scope('block4'):\n",
    "        conv19 = tf.layers.conv2d(\n",
    "                inputs=combine7,\n",
    "                filters=res_layers[3],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(reg),\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "        conv20 = tf.layers.conv2d(\n",
    "                inputs=conv19,\n",
    "                filters=res_layers[3],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(reg),\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "        conv21 = tf.layers.conv2d(\n",
    "                inputs=conv20,\n",
    "                filters=res_layers[3],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(reg),\n",
    "                activation=None)\n",
    "\n",
    "        combine8 = tf.nn.relu(tf.add(conv19, conv21))\n",
    "\n",
    "        conv22 = tf.layers.conv2d(\n",
    "                inputs=combine8,\n",
    "                filters=res_layers[3],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(reg),\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "        conv23 = tf.layers.conv2d(\n",
    "                inputs=conv22,\n",
    "                filters=res_layers[3],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(reg),\n",
    "                activation=None)\n",
    "\n",
    "        combine9 = tf.nn.relu(tf.add(conv23, combine8))\n",
    "\n",
    "        conv24 = tf.layers.conv2d(\n",
    "                inputs=combine9,\n",
    "                filters=res_layers[3],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(reg),\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "        conv25 = tf.layers.conv2d(\n",
    "                inputs=conv24,\n",
    "                filters=res_layers[3],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(reg),\n",
    "                activation=None,\n",
    "                name='final_layer')\n",
    "\n",
    "        combine10 = tf.nn.relu(tf.add(conv25, combine9))\n",
    "    \n",
    "    avepool = tf.layers.average_pooling2d(inputs=combine10, \n",
    "                                       pool_size=[2, 2], \n",
    "                                       strides=2)\n",
    "        \n",
    "    pool_flat = tf.contrib.layers.flatten(avepool, scope='pool2flat')\n",
    "    dense1 = tf.layers.dense(inputs=pool_flat, units=dense_layers, \n",
    "                             kernel_regularizer=tf.contrib.layers.l2_regularizer(reg), activation=tf.nn.relu)\n",
    "    dense1 = tf.nn.dropout(dense1, keep_prob, seed=2)\n",
    "    logits = tf.layers.dense(inputs=dense1, units=10)\n",
    "    return logits\n",
    "\n",
    "\n",
    "def apply_classification_loss_ResNet(model_function, reg, res_layers, dense_layers, lr, epsi):\n",
    "    with tf.Graph().as_default() as g:\n",
    "        with tf.device(\"/gpu:0\"):\n",
    "            x_ = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "            y_ = tf.placeholder(tf.int32, [None])\n",
    "            keep_prob = tf.placeholder(tf.float32)\n",
    "            \n",
    "            y_logits = model_function(x_, keep_prob, reg, res_layers, dense_layers)\n",
    "            \n",
    "            y_dict = dict(labels=y_, logits=y_logits)\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(**y_dict)\n",
    "            cross_entropy_loss = tf.reduce_mean(losses)\n",
    "            trainer = tf.train.AdamOptimizer(learning_rate=lr, beta1=0.9, beta2=0.999, epsilon=epsi)\n",
    "            train_op = trainer.minimize(cross_entropy_loss)\n",
    "            \n",
    "            y_pred = tf.argmax(tf.nn.softmax(y_logits), dimension=1)\n",
    "            correct_prediction = tf.equal(tf.cast(y_pred, tf.int32), y_)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = 'accuracy')\n",
    "    \n",
    "    model_dict = {'graph': g, 'inputs': [x_, y_, keep_prob], \n",
    "                  'train_op': train_op, 'accuracy': accuracy, 'loss': cross_entropy_loss}\n",
    "    \n",
    "    return model_dict\n",
    "\n",
    "def train_model_ResNet(model_dict, dataset_generators, epoch_n, \n",
    "                         keep_prob, print_every=287,save_model=False, load_model=False):\n",
    "    \n",
    "    with model_dict['graph'].as_default(), tf.Session() as sess:\n",
    "        # Define sumaries\n",
    "        tf.summary.scalar('accuracy', model_dict['accuracy'])\n",
    "        tf.summary.scalar('loss', model_dict['loss'])\n",
    "        \n",
    "        kernel = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'conv1')[0]\n",
    "        grid = put_kernels_on_grid (kernel)\n",
    "        tf.summary.image('conv1/features', grid, max_outputs=1)\n",
    "    \n",
    "        tf.summary.histogram('filter_weight', tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'conv1')[0])\n",
    "        tf.summary.histogram('filter_bias', tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'conv1')[1])\n",
    "        merged = tf.summary.merge_all()\n",
    "        # Initial writer\n",
    "        train_writer = tf.summary.FileWriter('./graph' + '/train',\n",
    "                                      sess.graph)\n",
    "        test_writer = tf.summary.FileWriter('./graph' + '/test')\n",
    "    \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'conv1'))\n",
    "            \n",
    "        if load_model:\n",
    "            saver.restore(sess, 'checkpoints/checkpoint.ckpt')\n",
    "            print('Model loaded')\n",
    "        \n",
    "        loss_tmp = 1000; acc_tmp = 0; patience_ct = 0\n",
    "        for epoch_i in range(epoch_n):\n",
    "            for iter_i, data_batch in enumerate(dataset_generators['train']):\n",
    "                train_feed_dict = dict(zip(model_dict['inputs'][:2], data_batch))\n",
    "                train_feed_dict[model_dict['inputs'][2]] = keep_prob\n",
    "                sess.run(model_dict['train_op'], feed_dict=train_feed_dict)\n",
    "                \n",
    "                # Vistualize training\n",
    "                summary_train = sess.run(merged, feed_dict=train_feed_dict)\n",
    "                train_writer.add_summary(summary_train, epoch_i) \n",
    "                \n",
    "                if iter_i % print_every == print_every-1:\n",
    "                    collect_arr = []\n",
    "                    for test_batch in dataset_generators['test']:\n",
    "                        test_feed_dict = dict(zip(model_dict['inputs'][:2], test_batch))\n",
    "                        test_feed_dict[model_dict['inputs'][2]] = 1\n",
    "                        to_compute = [model_dict['loss'], model_dict['accuracy']]\n",
    "                        collect_arr.append(sess.run(to_compute, test_feed_dict)) \n",
    "                        \n",
    "                        # Vistualize testing\n",
    "                        summary_test = sess.run(merged, feed_dict=test_feed_dict)\n",
    "                        test_writer.add_summary(summary_test, epoch_i)\n",
    "                        \n",
    "                    averages = np.mean(collect_arr, axis=0)\n",
    "                    fmt = (epoch_i+1, print_every, ) + tuple(averages)\n",
    "                    print('iteration {:d} {:d}\\t loss: {:.3f}, '\n",
    "                          'accuracy: {:.3f}'.format(*fmt))\n",
    "                    \n",
    "            # Early stopping with patience of 3 epoches\n",
    "            if averages[1] < acc_tmp:\n",
    "                patience_ct += 1\n",
    "                if patience_ct == 2:\n",
    "                    print('Early stopping!'); break\n",
    "            else: patience_ct = 0\n",
    "            loss_tmp = averages[0]; acc_tmp = averages[1]\n",
    "\n",
    "def SVHN_plusplus_ResNet(keep_prob, reg, res_layers, dense_layers, lr, epsi):\n",
    "    model_dict = apply_classification_loss_ResNet(cnn_map_ResNet, reg, res_layers, dense_layers, lr, epsi)\n",
    "    train_model_ResNet(model_dict, dataset_generators, 30, # set to 5 for tunning \n",
    "                         keep_prob, load_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid: 32 = (4, 8)\n",
      "Model loaded\n",
      "iteration 1 287\t loss: 0.594, accuracy: 0.822\n",
      "iteration 2 287\t loss: 0.441, accuracy: 0.869\n",
      "iteration 3 287\t loss: 0.340, accuracy: 0.904\n",
      "iteration 4 287\t loss: 0.289, accuracy: 0.918\n",
      "iteration 5 287\t loss: 0.273, accuracy: 0.925\n",
      "iteration 6 287\t loss: 0.264, accuracy: 0.931\n",
      "iteration 7 287\t loss: 0.261, accuracy: 0.935\n",
      "iteration 8 287\t loss: 0.281, accuracy: 0.933\n",
      "iteration 9 287\t loss: 0.285, accuracy: 0.935\n",
      "iteration 10 287\t loss: 0.260, accuracy: 0.938\n",
      "iteration 11 287\t loss: 0.303, accuracy: 0.933\n",
      "iteration 12 287\t loss: 0.298, accuracy: 0.935\n",
      "iteration 13 287\t loss: 0.301, accuracy: 0.939\n",
      "iteration 14 287\t loss: 0.294, accuracy: 0.936\n",
      "iteration 15 287\t loss: 0.327, accuracy: 0.937\n",
      "iteration 16 287\t loss: 0.329, accuracy: 0.939\n",
      "iteration 17 287\t loss: 0.327, accuracy: 0.938\n",
      "iteration 18 287\t loss: 0.360, accuracy: 0.936\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "SVHN_plusplus_ResNet(0.5, # dropout\n",
    "                     0.0005, # L2-regularizer\n",
    "                     [64,96,128,160], # Resnet filter size\n",
    "                     400, # dense layer width\n",
    "                     0.0005, # learning rate\n",
    "                     1e-8) # Adam epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Even though it has much higher accuray than 3-layer cnn, but it was very time consuming to train. And overfitting might prevent it to learn effectively. However, it has great potential if tuned right, as proven in the paper\n",
    " \" Deep residual learning for image recognition\".__\n",
    " \n",
    "<img width='500px' src=\"https://raw.githubusercontent.com/GordonCai/BU-EC500K-Deep-Learning/master/Homework/hw4/Vistualization/res%20loss%20accuracy.jpeg\">\n",
    "\n",
    "<img width='600px' src=\"https://raw.githubusercontent.com/GordonCai/BU-EC500K-Deep-Learning/master/Homework/hw4/Vistualization/res%20image.jpeg\">\n",
    "\n",
    "<img width='300px' src=\"https://raw.githubusercontent.com/GordonCai/BU-EC500K-Deep-Learning/master/Homework/hw4/Vistualization/res%20hist.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 shallow VGGNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width='400px' src=\"https://raw.githubusercontent.com/GordonCai/BU-EC500K-Deep-Learning/master/Homework/hw4/Vistualization/vgg%20graph%201.jpeg\">\n",
    "\n",
    "<img width='400px' src=\"https://raw.githubusercontent.com/GordonCai/BU-EC500K-Deep-Learning/master/Homework/hw4/Vistualization/vgg%20graph%202.jpeg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cnn_map_VGGNet(x_, keep_prob, reg, vgg_layers, dense_layers):\n",
    "    # Block 1\n",
    "    conv1 = tf.layers.conv2d(\n",
    "            inputs=x_,\n",
    "            filters=32,\n",
    "            kernel_size=[5, 5],\n",
    "            padding=\"same\",\n",
    "            kernel_regularizer = tf.contrib.layers.l2_regularizer(reg),\n",
    "            activation=tf.nn.relu,\n",
    "            name='conv1')\n",
    "\n",
    "    conv2 = tf.layers.conv2d(\n",
    "            inputs=conv1,\n",
    "            filters=32,\n",
    "            kernel_size=[3, 3],\n",
    "            padding=\"same\",\n",
    "            kernel_regularizer = tf.contrib.layers.l2_regularizer(reg),\n",
    "            activation=tf.nn.relu)\n",
    "\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv2, \n",
    "                                    pool_size=[2, 2], \n",
    "                                    strides=2)\n",
    "    \n",
    "    # Block 2\n",
    "    with tf.variable_scope('block2'):\n",
    "        conv3 = tf.layers.conv2d(\n",
    "                inputs=pool1,\n",
    "                filters=vgg_layers[0],\n",
    "                kernel_size=[5, 5],\n",
    "                padding=\"same\",\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(reg),\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "        conv4 = tf.layers.conv2d(\n",
    "                inputs=conv3,\n",
    "                filters=vgg_layers[0],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(reg),\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "        pool2 = tf.layers.max_pooling2d(inputs=conv4, \n",
    "                                        pool_size=[2, 2], \n",
    "                                        strides=2)\n",
    "    \n",
    "    # Block 3\n",
    "    with tf.variable_scope('block3'):\n",
    "        conv5 = tf.layers.conv2d(\n",
    "                inputs=pool2,\n",
    "                filters=vgg_layers[1],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(reg),\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "        conv6 = tf.layers.conv2d(\n",
    "                inputs=conv5,\n",
    "                filters=vgg_layers[1],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(reg),\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "        conv7 = tf.layers.conv2d(\n",
    "                inputs=conv6,\n",
    "                filters=vgg_layers[1],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(reg),\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "        pool3 = tf.layers.max_pooling2d(inputs=conv7, \n",
    "                                        pool_size=[2, 2], \n",
    "                                        strides=2)\n",
    "    \n",
    "    # Block 4\n",
    "    with tf.variable_scope('block4'):\n",
    "        conv8 = tf.layers.conv2d(\n",
    "                inputs=pool3,\n",
    "                filters=vgg_layers[2],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(reg),\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "        conv9 = tf.layers.conv2d(\n",
    "                inputs=conv8,\n",
    "                filters=vgg_layers[2],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(reg),\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "        conv10 = tf.layers.conv2d(\n",
    "                inputs=conv9,\n",
    "                filters=vgg_layers[2],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(reg),\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "        pool4 = tf.layers.max_pooling2d(inputs=conv10, \n",
    "                                        pool_size=[2, 2], \n",
    "                                        strides=2)\n",
    "    \n",
    "    # Block 5\n",
    "    with tf.variable_scope('block5'):\n",
    "        conv11 = tf.layers.conv2d(\n",
    "                inputs=pool4,\n",
    "                filters=vgg_layers[3],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(reg),\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "        conv12 = tf.layers.conv2d(\n",
    "                inputs=conv11,\n",
    "                filters=vgg_layers[3],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(reg),\n",
    "                activation=tf.nn.relu)\n",
    "\n",
    "        conv13 = tf.layers.conv2d(\n",
    "                inputs=conv12,\n",
    "                filters=vgg_layers[3],\n",
    "                kernel_size=[3, 3],\n",
    "                padding=\"same\",\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(reg),\n",
    "                activation=tf.nn.relu,\n",
    "                name='final_layer')\n",
    "\n",
    "        pool5 = tf.layers.max_pooling2d(inputs=conv13, \n",
    "                                        pool_size=[2, 2], \n",
    "                                        strides=2)\n",
    "        \n",
    "    pool_flat = tf.contrib.layers.flatten(pool5, scope='pool2flat')\n",
    "    dense1 = tf.layers.dense(inputs=pool_flat, units=dense_layers[0], \n",
    "                             kernel_initializer=tf.random_uniform_initializer(minval=-6*np.sqrt(1.0/(2000+dense_layers[0])), \n",
    "                                                                             maxval=6*np.sqrt(1.0/(2000+dense_layers[0])), seed=21), \n",
    "                             bias_initializer=tf.truncated_normal_initializer(mean=0.1, stddev=1e-4),\n",
    "                             kernel_regularizer=tf.contrib.layers.l2_regularizer(reg), activation=tf.nn.relu)\n",
    "    dense1 = tf.nn.dropout(dense1, keep_prob, seed=2)\n",
    "    dense2 = tf.layers.dense(inputs=dense1, units=dense_layers[1], \n",
    "                             kernel_initializer=tf.random_uniform_initializer(minval=-6*np.sqrt(1.0/(dense_layers[1]+dense_layers[0])), \n",
    "                                                                             maxval=6*np.sqrt(1.0/(dense_layers[1]+dense_layers[0])), seed=11),\n",
    "                             bias_initializer=tf.truncated_normal_initializer(mean=0.1, stddev=1e-4),\n",
    "                             kernel_regularizer=tf.contrib.layers.l2_regularizer(reg), activation=tf.nn.relu)\n",
    "    dense3 = tf.layers.dense(inputs=dense2, units=dense_layers[2], \n",
    "                             kernel_initializer=tf.random_uniform_initializer(minval=-6*np.sqrt(1.0/(dense_layers[1]+dense_layers[2])), \n",
    "                                                                             maxval=6*np.sqrt(1.0/(dense_layers[1]+dense_layers[2])), seed=1),\n",
    "                             kernel_regularizer=tf.contrib.layers.l2_regularizer(reg), activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(inputs=dense3, units=10)\n",
    "    return logits\n",
    "\n",
    "\n",
    "def apply_classification_loss_VGGNet(model_function, reg, vgg_layers, dense_layers, lr, epsi):\n",
    "    with tf.Graph().as_default() as g:\n",
    "        with tf.device(\"/gpu:0\"):\n",
    "            x_ = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "            y_ = tf.placeholder(tf.int32, [None])\n",
    "            keep_prob = tf.placeholder(tf.float32)\n",
    "            y_logits = model_function(x_, keep_prob, reg, vgg_layers, dense_layers)\n",
    "            \n",
    "            y_dict = dict(labels=y_, logits=y_logits)\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(**y_dict)\n",
    "            cross_entropy_loss = tf.reduce_mean(losses)\n",
    "            trainer = tf.train.AdamOptimizer(learning_rate=lr, beta1=0.9, beta2=0.999, epsilon=epsi)\n",
    "            train_op = trainer.minimize(cross_entropy_loss)\n",
    "            \n",
    "            y_pred = tf.argmax(tf.nn.softmax(y_logits), dimension=1)\n",
    "            correct_prediction = tf.equal(tf.cast(y_pred, tf.int32), y_)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = 'accuracy')\n",
    "    \n",
    "    model_dict = {'graph': g, 'inputs': [x_, y_, keep_prob], 'train_op': train_op,\n",
    "                  'accuracy': accuracy, 'loss': cross_entropy_loss}\n",
    "    \n",
    "    return model_dict\n",
    "\n",
    "def train_model_VGGNet(model_dict, dataset_generators, epoch_n, keep_prob, print_every=287,\n",
    "                    save_model=False, load_model=False):\n",
    "    \n",
    "    with model_dict['graph'].as_default(), tf.Session() as sess:\n",
    "        # Define sumaries\n",
    "        tf.summary.scalar('accuracy', model_dict['accuracy'])\n",
    "        tf.summary.scalar('loss', model_dict['loss'])\n",
    "        \n",
    "        kernel = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'conv1')[0]\n",
    "        grid = put_kernels_on_grid (kernel)\n",
    "        tf.summary.image('conv1/features', grid, max_outputs=1)\n",
    "    \n",
    "        tf.summary.histogram('filter_weight', tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'conv1')[0])\n",
    "        tf.summary.histogram('filter_bias', tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'conv1')[1])\n",
    "        merged = tf.summary.merge_all()\n",
    "        # Initial writer\n",
    "        train_writer = tf.summary.FileWriter('./graph' + '/train',\n",
    "                                      sess.graph)\n",
    "        test_writer = tf.summary.FileWriter('./graph' + '/test')\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'conv1'))\n",
    "            \n",
    "        if load_model:\n",
    "            saver.restore(sess, 'checkpoints/checkpoint.ckpt')\n",
    "            print('Model loaded')\n",
    "        \n",
    "        loss_tmp = 1000; acc_tmp = 0; patience_ct = 0\n",
    "        for epoch_i in range(epoch_n):\n",
    "            for iter_i, data_batch in enumerate(dataset_generators['train']):\n",
    "                train_feed_dict = dict(zip(model_dict['inputs'][:2], data_batch))\n",
    "                train_feed_dict[model_dict['inputs'][2]] = keep_prob\n",
    "                sess.run(model_dict['train_op'], feed_dict=train_feed_dict)\n",
    "                \n",
    "                # Vistualize training\n",
    "                summary_train = sess.run(merged, feed_dict=train_feed_dict)\n",
    "                train_writer.add_summary(summary_train, epoch_i) \n",
    "                \n",
    "                if iter_i % print_every == print_every-1:\n",
    "                    collect_arr = []\n",
    "                    for test_batch in dataset_generators['test']:\n",
    "                        test_feed_dict = dict(zip(model_dict['inputs'][:2], test_batch))\n",
    "                        test_feed_dict[model_dict['inputs'][2]] = 1\n",
    "                        to_compute = [model_dict['loss'], model_dict['accuracy']]\n",
    "                        collect_arr.append(sess.run(to_compute, test_feed_dict)) \n",
    "                        \n",
    "                        # Vistualize testing\n",
    "                        summary_test = sess.run(merged, feed_dict=test_feed_dict)\n",
    "                        test_writer.add_summary(summary_test, epoch_i)\n",
    "                        \n",
    "                    averages = np.mean(collect_arr, axis=0)\n",
    "                    fmt = (epoch_i+1, print_every, ) + tuple(averages)\n",
    "                    print('iteration {:d} {:d}\\t loss: {:.3f}, '\n",
    "                          'accuracy: {:.3f}'.format(*fmt))\n",
    "                    \n",
    "            # Early stopping with patience of 2 epoches\n",
    "            if averages[1] < acc_tmp:\n",
    "                patience_ct += 1\n",
    "                if patience_ct == 2:\n",
    "                    print('Early stopping!'); break\n",
    "            else: patience_ct = 0\n",
    "            loss_tmp = averages[0]; acc_tmp = averages[1]\n",
    "\n",
    "def SVHN_plusplus_VGGNet(keep_prob, reg, vgg_layers, dense_layers, lr, epsi):\n",
    "    model_dict = apply_classification_loss_VGGNet(cnn_map_VGGNet, reg, vgg_layers, dense_layers, lr, epsi)\n",
    "    train_model_VGGNet(model_dict, dataset_generators, 50, keep_prob,\n",
    "                       print_every=287, save_model=False, load_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid: 32 = (4, 8)\n",
      "Model loaded\n",
      "iteration 1 287\t loss: 0.504, accuracy: 0.849\n",
      "iteration 2 287\t loss: 0.381, accuracy: 0.895\n",
      "iteration 3 287\t loss: 0.348, accuracy: 0.904\n",
      "iteration 4 287\t loss: 0.304, accuracy: 0.920\n",
      "iteration 5 287\t loss: 0.326, accuracy: 0.911\n",
      "iteration 6 287\t loss: 0.302, accuracy: 0.920\n",
      "iteration 7 287\t loss: 0.308, accuracy: 0.920\n",
      "iteration 8 287\t loss: 0.277, accuracy: 0.927\n",
      "iteration 9 287\t loss: 0.288, accuracy: 0.928\n",
      "iteration 10 287\t loss: 0.315, accuracy: 0.921\n",
      "iteration 11 287\t loss: 0.302, accuracy: 0.930\n",
      "iteration 12 287\t loss: 0.320, accuracy: 0.927\n",
      "iteration 13 287\t loss: 0.316, accuracy: 0.933\n",
      "iteration 14 287\t loss: 0.345, accuracy: 0.930\n",
      "iteration 15 287\t loss: 0.337, accuracy: 0.929\n",
      "Early stopping!\n"
     ]
    }
   ],
   "source": [
    "SVHN_plusplus_VGGNet(0.7, # dropout\n",
    "                     0.005, # L2-regularizer\n",
    "                     [64,128,256,512], # # VGGnet blocks' filter size\n",
    "                     [1000,500,100], # 3 dense layers' width\n",
    "                     0.0006, # learning rate\n",
    "                     1e-8) # Adam epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__This VGGNet was much faster to train and less memory consuming than RESNet, and yielded less accuracy. But it was still an improvement over vanilla CNN and AlexNet.__\n",
    "\n",
    "<img width='400px' src=\"https://raw.githubusercontent.com/GordonCai/BU-EC500K-Deep-Learning/master/Homework/hw4/Vistualization/vgg%20loss%20accuracy.jpeg\">\n",
    "\n",
    "<img width='600px' src=\"https://raw.githubusercontent.com/GordonCai/BU-EC500K-Deep-Learning/master/Homework/hw4/Vistualization/vgg%20image.jpeg\">\n",
    "\n",
    "<img width='300px' src=\"https://raw.githubusercontent.com/GordonCai/BU-EC500K-Deep-Learning/master/Homework/hw4/Vistualization/vgg%20hist.jpeg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
