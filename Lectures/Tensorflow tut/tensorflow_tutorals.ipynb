{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow examples - simplified\n",
    "\n",
    "Hi there,\n",
    "\n",
    "I do really love tensorflow [official documentation](https://www.tensorflow.org/versions/r0.7/how_tos/index.html), [API reference](https://www.tensorflow.org/versions/r0.7/api_docs/index.html) and [tutorials](https://www.tensorflow.org/versions/r0.7/tutorials/index.html), but I (personally) found [official code examples](https://github.com/tensorflow/tensorflow/tree/r0.7/tensorflow/examples/tutorials) to be annoyingly [complex](https://en.wikipedia.org/wiki/Cyclomatic_complexity). This file is an attempt to rewrite those examples so that anyone could read them from-bottom-to-top and get a (somewhat) full understanding of what is happening. Many parts are ommited or simplified just beacuse it seemed to me that it's more natural to structure code this way.\n",
    "\n",
    "This could also be use as the code you look at while reading an official tutorial.\n",
    "\n",
    "Warning: I'm a total tensorflow newby and this file reflects my newby's understanding of the subject - that could have little in common with reality :)\n",
    "\n",
    "This is not a final version, though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic MINST Model (aka [MNIST For ML Beginners](https://www.tensorflow.org/versions/r0.7/tutorials/mnist/beginners/index.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"../data/MNIST_tf_data/\", one_hot=True)\n",
    "\n",
    "## placeholders - like theano.tensor.tensor()\n",
    "## each time you run(vals=[vals]) some expression you need to specify\n",
    "## values for the placeholders \n",
    "x_train = tf.placeholder(tf.float32, [None, 784])\n",
    "y_train = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "## variables - like theano.shared()\n",
    "## retain values between calls \n",
    "## within a single Session() of execution\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "y_pred = tf.nn.softmax(tf.matmul(x_train, W) + b)\n",
    "\n",
    "\n",
    "## expression - can be _evaluated_\n",
    "## (maps [vars] -> result, changes nothing - pure function)\n",
    "cross_entropy = -tf.reduce_sum(y_train * tf.log(y_pred))\n",
    "y_train_eq_pred = tf.equal(tf.argmax(y_train,1), tf.argmax(y_pred,1))\n",
    "accuracy_exp = tf.reduce_mean(tf.cast(y_train_eq_pred, tf.float32))\n",
    "\n",
    "## operation - can be _executed_ to updates values of Variables\n",
    "## (maps [vars] -> None, updates state)\n",
    "init_op = tf.initialize_all_variables()\n",
    "train_step_op = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.4075\n",
      "1000 0.916\n",
      "2000 0.9127\n",
      "3000 0.9228\n",
      "4000 0.9203\n",
      "5000 0.9181\n",
      "6000 0.9208\n",
      "7000 0.9185\n",
      "8000 0.9229\n",
      "9000 0.9212\n"
     ]
    }
   ],
   "source": [
    "## releases resources (memory, etc.) in the end of the block\n",
    "with tf.Session() as s:\n",
    "    s.run(init_op)\n",
    "    for i in xrange(10000):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "        s.run(train_step_op, {x_train: batch_xs, y_train: batch_ys})\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print i, s.run(accuracy_exp, \n",
    "                           feed_dict={x_train: mnist.test.images, y_train: mnist.test.labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links:\n",
    "\n",
    "API:\n",
    "* [tf.placeholder](https://www.tensorflow.org/versions/r0.7/api_docs/python/io_ops.html#placeholders)\n",
    "* [tf.Variable](https://www.tensorflow.org/versions/r0.7/api_docs/python/state_ops.html#variables)\n",
    "* [tf.Operation](https://www.tensorflow.org/versions/r0.7/api_docs/python/framework.html#Operation)\n",
    "* [tf.Session](https://www.tensorflow.org/versions/r0.7/api_docs/python/client.html#session-management)\n",
    "\n",
    "How-to\n",
    "* https://www.tensorflow.org/versions/r0.7/get_started/basic_usage.html#overview\n",
    "* https://www.tensorflow.org/versions/r0.7/how_tos/reading_data/index.html#feeding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignore that:\n",
    "\n",
    "this is not tensorflow, but just some utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./mutils/config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./mutils/config.py\n",
    "## (ignore two cell below - that is not part of tf)\n",
    "## If you want to run the code in this cell with without\n",
    "## writing anywhere, just comment %%writefile and run the cell\n",
    "\n",
    "class Immutable(type):\n",
    "    def __init__(cls, name, bases, dct):\n",
    "        def tmp(*a, **b):\n",
    "            raise RuntimeError, (\"This is config, don't create objects, just use it!\")\n",
    "        cls.__init__ = tmp\n",
    "\n",
    "    def __setattr__(cls, key, value):\n",
    "        if key != '__init__':\n",
    "            raise RuntimeError, (\"Configs are immutable! Don't do that.\")\n",
    "        else:\n",
    "            super(Immutable, cls).__setattr__(key, value)\n",
    "\n",
    "class BaseConfig:\n",
    "    __metaclass__ = Immutable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./mutils/record.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./mutils/record.py\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "def record(name, d):\n",
    "    return namedtuple(name, d.keys())(**d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Model (aka [TensorFlow Mechanics 101](https://www.tensorflow.org/versions/r0.7/tutorials/mnist/tf/index.html))\n",
    "\n",
    "Original code: [mnist.py](https://github.com/tensorflow/tensorflow/blob/r0.7/tensorflow/examples/tutorials/mnist/mnist.py) and [fully_connected_feed.py](https://github.com/tensorflow/tensorflow/blob/r0.7/tensorflow/examples/tutorials/mnist/fully_connected_feed.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.examples.tutorials.mnist import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mutils.config import BaseConfig\n",
    "\n",
    "## like namedtuple but with inheritance!\n",
    "class BasicConfig(BaseConfig):    \n",
    "    learning_rate = 0.01\n",
    "    max_steps = 2000\n",
    "    hidden1 = 128\n",
    "    hidden2 = 32\n",
    "    batch_size = 100\n",
    "    train_dir = '/home/usman/data/tf_tmp'\n",
    "    fake_data = False\n",
    "       \n",
    "## the cool thing about this config format is that \n",
    "## you can inhierit it for your specific case\n",
    "## and override default values like:\n",
    "## class CoolConfig(BasicConfig):\n",
    "##     max_steps = 3000\n",
    "\n",
    "config = BasicConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from mutils.record import record\n",
    "\n",
    "def hidden_layer(input_var, input_shape, hidden_units, name, activation=tf.nn.relu):\n",
    "    ## in scope A the variable a would have absolute name = 'A/a'\n",
    "    ## each layer definres it's own scope, variables are accessable \n",
    "    ## later via tf.get_variable('hidden1/w')\n",
    "    with tf.name_scope(name):\n",
    "        sigma = math.sqrt(float(hidden_units))\n",
    "        init_val = tf.truncated_normal([input_shape, hidden_units], stddev=1.0/sigma)\n",
    "        w = tf.Variable(initial_value=init_val, trainable=True, name='w')\n",
    "        biases = tf.Variable(tf.zeros([hidden_units]), name='b')\n",
    "        layer = activation(tf.matmul(input_var, w) + biases)\n",
    "    return layer\n",
    "\n",
    "def build_model(X_train_ps, y_train_ps, config):\n",
    "    ## first two args are tf.placeholder's\n",
    "    hidden1 = hidden_layer(X_train_ps, input_shape=mnist.IMAGE_PIXELS, hidden_units=100, \n",
    "                           name='hidden1')\n",
    "    hidden2 = hidden_layer(hidden1, input_shape=100, hidden_units=100, \n",
    "                           name='hidden2')\n",
    "    prob_y_pred = hidden_layer(hidden2, input_shape=100, hidden_units=10, \n",
    "                               name='logit', activation=lambda x: x)\n",
    "    \n",
    "    ## above code is eqvivalent to theano's: \n",
    "    ## hstack([labels.T, range(len(labels)).T])\n",
    "    ## [ [0], [1], [2], [3], ...]\n",
    "    indices = tf.expand_dims(tf.range(0, config.batch_size), 1)\n",
    "    ## labels -> [ [2], [5], [2], [1], ...]\n",
    "    y_train_ps_ = tf.expand_dims(y_train_ps, 1)\n",
    "    ## [ [0, 2], [1, 5], ... ]\n",
    "    y_train_ps_pairs = tf.concat(1, [indices, y_train_ps_])\n",
    "    \n",
    "    output_shape = tf.pack([config.batch_size, mnist.NUM_CLASSES])\n",
    "    onehot_y_train = tf.sparse_to_dense(sparse_indices=y_train_ps_pairs,\n",
    "                                        output_shape=output_shape, \n",
    "                                        sparse_values=1.0, default_value=0.0)\n",
    "    \n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "        prob_y_pred, onehot_y_train, name='xentropy')\n",
    "    loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "    ## score = sum(argmax(prob_y_pred) == y_i)\n",
    "    correct = tf.nn.in_top_k(prob_y_pred, y_train_ps, 1)\n",
    "    score = tf.reduce_sum(tf.cast(correct, tf.int32))\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(config.learning_rate)\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    \n",
    "    ## Add a scalar protobuf summary for the loss\n",
    "    tf.scalar_summary(\"loss\", loss)\n",
    "    ## there must (!) be at least one summary before this call\n",
    "    ## would cause an error otherwise\n",
    "    summary_op = tf.merge_all_summaries()\n",
    "\n",
    "    ## for variable values checkpoints \n",
    "    ## (automaticaly makes restarable copies on disk)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    ## to save/restore\n",
    "    ## save_path = saver.save(sess, path)\n",
    "    ## saver.restore(sess, path)\n",
    "    \n",
    "    ## see `record(..)` definition above\n",
    "    return record('Model', {\n",
    "        'train_op': train_op,\n",
    "        'loss': loss,\n",
    "        'score': score,\n",
    "        'summary_op': summary_op,\n",
    "        'saver': saver,\n",
    "        'config': config,\n",
    "    })\n",
    "\n",
    "# model = build_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(variables, model, data, verbose_every=100, validate_every=1000):\n",
    "    with tf.Session() as s:\n",
    "        ## writes provided summaries into ./dir/log\n",
    "        writer = tf.train.SummaryWriter(model.config.train_dir, \n",
    "                                        graph_def=s.graph_def)\n",
    "        \n",
    "        ## operation that initializes all variables in graph\n",
    "        s.run(tf.initialize_all_variables())\n",
    "        \n",
    "        for step in xrange(model.config.max_steps):\n",
    "            start_time = time.time()\n",
    "            data_batch = data.train.next_batch(model.config.batch_size, \n",
    "                                               model.config.fake_data)\n",
    "            ## {X_train_ps: data_batch[0], y_train_ps: data_batch[1]}\n",
    "            data_dict = dict(zip(variables, data_batch)) \n",
    "            ## train_op() -> None, model.loss() -> loss_value\n",
    "            _, loss_value = s.run([model.train_op, model.loss], data_dict)\n",
    "            duration = time.time() - start_time\n",
    "            \n",
    "            if verbose_every > 0 and step % verbose_every == 0:\n",
    "                print('Step %d: loss = %.2f (%.3f sec)' % (step, \n",
    "                                                           loss_value, \n",
    "                                                           duration))\n",
    "                \n",
    "                ## failed to make following two lines work properly (?):\n",
    "                # log_msg = s.run(model.summary_op, feed_dict=data_dict)\n",
    "                # writer.add_summary(log_msg, step)\n",
    "            \n",
    "            # validate_every + after last iteration\n",
    "            if (  (validate_every > 0 and step % validate_every == 0) or\n",
    "                  (step == model.config.max_steps - 1) ):\n",
    "                model.saver.save(s, model.config.train_dir, global_step=step)\n",
    "                validation_sets_list = [ ('train', data.train), \n",
    "                                         ('test', data.test), \n",
    "                                         ('validate', data.validation)]\n",
    "                for dataset_name, data_set in validation_sets_list:\n",
    "                    n_batches = data_set.num_examples // model.config.batch_size\n",
    "                    n_samples = n_batches*model.config.batch_size\n",
    "                    score_sum = 0\n",
    "                    for i in range(n_batches):\n",
    "                        data_batch = data_set.next_batch(model.config.batch_size, \n",
    "                                                         model.config.fake_data)\n",
    "                        data_dict = dict(zip(variables, data_batch)) \n",
    "                        score_sum += s.run(model.score, data_dict)\n",
    "                    print('On \"%s\" dataset score is %.3f' % (dataset_name, \n",
    "                                                           float(score_sum) / n_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/usman/data/tf_tmp/train-images-idx3-ubyte.gz\n",
      "Extracting /home/usman/data/tf_tmp/train-labels-idx1-ubyte.gz\n",
      "Extracting /home/usman/data/tf_tmp/t10k-images-idx3-ubyte.gz\n",
      "Extracting /home/usman/data/tf_tmp/t10k-labels-idx1-ubyte.gz\n",
      "Step 0: loss = 2.62 (0.108 sec)\n",
      "On \"train\" dataset score is 0.067\n",
      "On \"test\" dataset score is 0.062\n",
      "On \"validate\" dataset score is 0.072\n",
      "Step 100: loss = 1.34 (0.020 sec)\n",
      "Step 200: loss = 0.76 (0.028 sec)\n",
      "Step 300: loss = 0.60 (0.024 sec)\n",
      "Step 400: loss = 0.54 (0.024 sec)\n",
      "Step 500: loss = 0.43 (0.012 sec)\n",
      "Step 600: loss = 0.61 (0.032 sec)\n",
      "Step 700: loss = 0.42 (0.040 sec)\n",
      "Step 800: loss = 0.29 (0.068 sec)\n",
      "Step 900: loss = 0.47 (0.020 sec)\n",
      "Step 1000: loss = 0.48 (0.012 sec)\n",
      "On \"train\" dataset score is 0.893\n",
      "On \"test\" dataset score is 0.899\n",
      "On \"validate\" dataset score is 0.906\n",
      "Step 1100: loss = 0.30 (0.772 sec)\n",
      "Step 1200: loss = 0.15 (0.028 sec)\n",
      "Step 1300: loss = 0.21 (0.040 sec)\n",
      "Step 1400: loss = 0.39 (0.032 sec)\n",
      "Step 1500: loss = 0.26 (0.028 sec)\n",
      "Step 1600: loss = 0.23 (0.044 sec)\n",
      "Step 1700: loss = 0.14 (0.072 sec)\n",
      "Step 1800: loss = 0.29 (0.016 sec)\n",
      "Step 1900: loss = 0.28 (0.004 sec)\n",
      "On \"train\" dataset score is 0.914\n",
      "On \"test\" dataset score is 0.917\n",
      "On \"validate\" dataset score is 0.925\n"
     ]
    }
   ],
   "source": [
    "def run():\n",
    "    X_train_ps = tf.placeholder(tf.float32, shape=(config.batch_size, mnist.IMAGE_PIXELS))\n",
    "    y_train_ps = tf.placeholder(tf.int32, shape=(config.batch_size))\n",
    "    \n",
    "    model = build_model(X_train_ps, y_train_ps, config)\n",
    "    data = input_data.read_data_sets(config.train_dir, config.fake_data)\n",
    "    \n",
    "    train(variables=(X_train_ps, y_train_ps), \n",
    "          model=model, \n",
    "          data=data)\n",
    "    \n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links\n",
    "\n",
    "API:\n",
    "\n",
    "* [tf.Graph](https://www.tensorflow.org/versions/r0.7/api_docs/python/framework.html#Graph)\n",
    "* [tf.name_scope](https://www.tensorflow.org/versions/r0.7/api_docs/python/framework.html#name_scope) and [tf.variable_scope](https://www.tensorflow.org/versions/r0.7/api_docs/python/state_ops.html#variable_scope) and what is the [difference](http://stackoverflow.com/questions/34215746/difference-between-variable-scope-and-name-scope-in-tensorflow) between them?\n",
    "* [tf.train.Optimizer](https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#optimizers)\n",
    "* [Summary Operations](https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#summary-operations) (see [Protocol Buffers](https://en.wikipedia.org/wiki/Protocol_Buffers) aka protobuf) and [tf.train.SummaryWriter](https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#SummaryWriter)\n",
    "* [tf.train.Saver](https://www.tensorflow.org/versions/r0.7/api_docs/python/state_ops.html#Saver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## RNN Basics (aka [RNN tutorial](https://www.tensorflow.org/versions/r0.7/tutorials/recurrent/index.html))\n",
    "\n",
    "Original [code](https://github.com/tensorflow/tensorflow/tree/r0.7/tensorflow/models/rnn/ptb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mutils.config import BaseConfig\n",
    "\n",
    "class SmallConfig(BaseConfig):\n",
    "    # MODEL HYPERPARAMS:\n",
    "    num_layers = 2\n",
    "    num_steps = 20 # maximum sequence length in a batch\n",
    "    hidden_size = 200\n",
    "    vocab_size = 10000\n",
    "    dropout_prob = 0.0\n",
    "    \n",
    "    # TRANING:\n",
    "    max_epoch = 13\n",
    "    batch_size = 20\n",
    "    learning_rate = 1.0\n",
    "    grad_clip_norm = 5\n",
    "    ## start slowing down learning starting from slowdown_epoch e.g. \n",
    "    ## real_learning_rate = learning_rate * lr_decay**(i - slowdown_epoch)\n",
    "    slowdown_epoch = 4\n",
    "    lr_decay = 0.5\n",
    "    init_scale = 0.1 # std of weight initialization\n",
    "    \n",
    "    data_path = '../data/tf_langmodel/simple-examples/data'\n",
    "    \n",
    "## on evaluation step we need to process \n",
    "## data lines one-by-one; override settings\n",
    "class EvaluationConfig(SmallConfig):\n",
    "    batch_size = 1\n",
    "    num_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.models.rnn import rnn_cell\n",
    "from tensorflow.models.rnn import seq2seq\n",
    "from mutils.record import record\n",
    "\n",
    "def build_model(X_placeholder, Y_placeholder, config, is_training=True):\n",
    "    \"\"\"\n",
    "    X_placeholder and Y_placeholder\n",
    "        - of shape [batch_size, num_steps]\n",
    "    \"\"\"\n",
    "    is_using_dropout = is_training and config.dropout_prob > 0\n",
    "    \n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        ## tf.get_variable - creates Variable() in current Scope() _if does not exit_\n",
    "        embedding_matrix = tf.get_variable(\"embedding_matrix\", [config.vocab_size, \n",
    "                                                                config.hidden_size])\n",
    "        X_embedded = tf.nn.embedding_lookup(embedding_matrix, X_placeholder)\n",
    "        ## - of shape [batch_size, num_steps, hidden_size]\n",
    "    X_embedded_dropped = (tf.nn.dropout(X_embedded, 1-config.dropout_prob)\n",
    "                          if is_using_dropout\n",
    "                          else X_embedded)\n",
    "    \n",
    "    ## forget_bias > 0 - better results, but paper (ptb) uses forget_bias == 0\n",
    "    lstm_cell = rnn_cell.BasicLSTMCell(config.hidden_size, forget_bias=0.0)\n",
    "    lstm_cell_dropped = (rnn_cell.DropoutWrapper(lstm_cell, \n",
    "                                                 1-config.dropout_prob)\n",
    "                         if is_using_dropout\n",
    "                         else lstm_cell)\n",
    "    cell = rnn_cell.MultiRNNCell([lstm_cell_dropped] * config.num_layers)\n",
    "    \n",
    "    ## in real-world scenario use tensorflow.models.rnn.rnn.py\n",
    "    tutorial_mode = True\n",
    "    \n",
    "    if tutorial_mode:\n",
    "        rnn_input = X_embedded_dropped\n",
    "        outputs = []\n",
    "        # [batch_size, hidden_size]\n",
    "        init_state = cell.zero_state(config.batch_size, tf.float32)\n",
    "        state = init_state\n",
    "        with tf.variable_scope(\"RNN\"):\n",
    "            for time_step in range(config.num_steps):\n",
    "                ## forces to use _same scope_ with _same variables_ as on t=0\n",
    "                if time_step > 0:\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "                (cell_output, state) = cell(rnn_input[:, time_step, :], state)\n",
    "                outputs.append(cell_output)\n",
    "    else:\n",
    "        from tensorflow.models.rnn import rnn\n",
    "        inputs = [tf.squeeze(input_slice, squeeze_dims=[1]) # shape[len, 1] -> shape[len]\n",
    "                  for input_slice \n",
    "                  in tf.split(split_dim=1, num_split=num_steps, value=inputs)]\n",
    "        init_state = cell.zero_state(config.batch_size, tf.float32)\n",
    "        outputs, state = rnn.rnn(cell, inputs, initial_state=init_state)\n",
    "    \n",
    "    final_state = state\n",
    "\n",
    "    rnn_output = tf.reshape(tf.concat(concat_dim=1, values=outputs), \n",
    "                            [-1, config.hidden_size])\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [config.hidden_size, config.vocab_size])\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [config.vocab_size])\n",
    "    logits = tf.matmul(rnn_output, softmax_w) + softmax_b\n",
    "    ## loss_arr: log-perplexity (cross-entropy) of shape [seq_len, ]\n",
    "    loss_arr = seq2seq.sequence_loss_by_example(logits=[logits],\n",
    "                                                targets=[tf.reshape(Y_placeholder, [-1])],\n",
    "                                                weights=[tf.ones([config.batch_size * \n",
    "                                                                  config.num_steps])])\n",
    "    loss = tf.reduce_sum(loss_arr) / config.batch_size\n",
    "\n",
    "    if not is_training:\n",
    "        return\n",
    "\n",
    "    learning_rate = tf.Variable(0.0, trainable=False)\n",
    "    model_weights = tf.trainable_variables()\n",
    "    grads = tf.gradients(loss, model_weights)\n",
    "    grads_clipped, norms = tf.clip_by_global_norm(grads, clip_norm=config.grad_clip_norm)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    ## we use custom compute_gradients() => use apply_gradients(), not minimize()\n",
    "    train_op = optimizer.apply_gradients(zip(grads_clipped, model_weights))\n",
    "    \n",
    "    return record('Model', {\n",
    "        'train_op': train_op,\n",
    "        'loss': loss,\n",
    "        'learning_rate': learning_rate,\n",
    "        'config': config,\n",
    "        'init_state': init_state,\n",
    "        'final_state': final_state,\n",
    "    })\n",
    "\n",
    "def test():\n",
    "    ## to avoid matrix re-definition errors in tf.get_variable()\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    config = SmallConfig\n",
    "    X_p = tf.placeholder(tf.int32, [config.batch_size, config.num_steps])\n",
    "    Y_p = tf.placeholder(tf.int32, [config.batch_size, config.num_steps])\n",
    "    model = build_model(X_p, Y_p, config)\n",
    "    \n",
    "# test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## if you're interested in dataparsing (I'm not), see:\n",
    "from tensorflow.models.rnn.ptb import reader\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def train(vars, model, data):\n",
    "    ## if you use tf.Session().as_default()\n",
    "    ##  - you replace tf.defaul_session object and can do\n",
    "    ## `expression.eval()` instead of `s.run(expression)`\n",
    "    ## BUT: resources won't be released after block ends\n",
    "    \n",
    "    ## if you do tf.Graph().as_default() - you need to \n",
    "    ## call build_model() _inside_ of this block, because\n",
    "    ## model is in old default graph and call() is new graph\n",
    "    with tf.Session() as s:\n",
    "        s.run(tf.initialize_all_variables())\n",
    "        \n",
    "        ## for each: epoch\n",
    "        for n_epoch in xrange(model.config.max_epoch):\n",
    "            print 'epoch %d of %d' % (n_epoch, model.config.max_epoch)\n",
    "            if n_epoch > model.config.slowdown_epoch:\n",
    "                ## start slowing down learning starting from slowdown_epoch\n",
    "                lr_decent = model.config.lr_decay ** (n_epoch - \n",
    "                                                      model.config.slowdown_epoch)\n",
    "                s.run(tf.assign(model.learning_rate, model.config.learning_rate * lr_decent))\n",
    "            dummy_op = tf.zeros([1, ], tf.int32) # does nothing\n",
    "            ## for each: dataset\n",
    "            for data_set, main_op, data_name in [(data.train, model.train_op, 'train'), \n",
    "                                                 (data.test, dummy_op, 'test')]:\n",
    "                data_iterator = reader.ptb_iterator(data_set, \n",
    "                                                    model.config.batch_size, \n",
    "                                                    model.config.num_steps)\n",
    "                \n",
    "                ## verbose output\n",
    "                n_batches = int(len(data_set) / \n",
    "                                model.config.batch_size / \n",
    "                                model.config.num_steps)\n",
    "                start_time = time.time()\n",
    "                ##\n",
    "                \n",
    "                state = s.run(model.init_state)\n",
    "                loss = 0\n",
    "                ## for each: batch\n",
    "                for batch_n, batch_data in enumerate(data_iterator):\n",
    "                    ## {x_placeholder: batch.x, .., init_state: state}\n",
    "                    data_dict = dict(zip(vars, batch_data))\n",
    "                    data_dict[model.init_state] = state\n",
    "                    computation_plan = [model.loss, model.final_state, main_op]\n",
    "                    batch_loss, state, _ = s.run(computation_plan, data_dict)\n",
    "                    loss += batch_loss\n",
    "                    \n",
    "                    ## verbose output\n",
    "                    if batch_n > 0 and batch_n % (n_batches / 10) == 0: # print 10 lines\n",
    "                        time_diff = time.time() - start_time\n",
    "                        batch_per_sec = float(batch_n) / time_diff\n",
    "                        time_remained = (n_batches - batch_n) / batch_per_sec\n",
    "                        print ('batch %d of %d for %.1f sec '\n",
    "                               '(%.1f batch per second; %.1f sec remained)' % (\n",
    "                                batch_n, n_batches, time_diff,\n",
    "                                batch_per_sec, time_remained)\n",
    "                              )\n",
    "                        \n",
    "                total_steps = model.config.num_steps*(batch_n+1)\n",
    "                loss_per_one = loss / total_steps\n",
    "                perplexity = np.exp(loss_per_one)\n",
    "\n",
    "                print (\"epoch %d, %s: unit loss\"\n",
    "                       \"is %.3f, peplexity: %.3f\" % (n_epoch, data_name, \n",
    "                                                     loss_per_one, perplexity))\n",
    "                \n",
    "            tick.tack() # end of epoch\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model\n",
      "7.87sec (7.87sec total)\n",
      "reading data\n",
      "1.60sec (9.47sec total)\n",
      "training\n",
      "epoch 0 of 13\n",
      "batch 232 of 2323 for 38.6 sec (6.0 batch per second; 347.9 sec remained)\n",
      "batch 464 of 2323 for 74.8 sec (6.2 batch per second; 299.6 sec remained)\n",
      "batch 696 of 2323 for 113.5 sec (6.1 batch per second; 265.4 sec remained)\n",
      "batch 928 of 2323 for 152.5 sec (6.1 batch per second; 229.3 sec remained)\n",
      "batch 1160 of 2323 for 191.7 sec (6.1 batch per second; 192.2 sec remained)\n",
      "batch 1392 of 2323 for 226.4 sec (6.1 batch per second; 151.4 sec remained)\n",
      "batch 1624 of 2323 for 257.3 sec (6.3 batch per second; 110.7 sec remained)\n",
      "batch 1856 of 2323 for 293.3 sec (6.3 batch per second; 73.8 sec remained)\n",
      "batch 2088 of 2323 for 330.9 sec (6.3 batch per second; 37.2 sec remained)\n",
      "batch 2320 of 2323 for 369.1 sec (6.3 batch per second; 0.5 sec remained)\n",
      "epoch 0, train: unit lossis 9.221, peplexity: 10110.956\n",
      "batch 20 of 206 for 0.9 sec (22.2 batch per second; 8.4 sec remained)\n",
      "batch 40 of 206 for 1.6 sec (24.5 batch per second; 6.8 sec remained)\n",
      "batch 60 of 206 for 2.4 sec (25.1 batch per second; 5.8 sec remained)\n",
      "batch 80 of 206 for 3.2 sec (25.1 batch per second; 5.0 sec remained)\n",
      "batch 100 of 206 for 4.0 sec (25.0 batch per second; 4.2 sec remained)\n",
      "batch 120 of 206 for 4.8 sec (24.8 batch per second; 3.5 sec remained)\n",
      "batch 140 of 206 for 5.7 sec (24.6 batch per second; 2.7 sec remained)\n",
      "batch 160 of 206 for 6.6 sec (24.4 batch per second; 1.9 sec remained)\n",
      "batch 180 of 206 for 7.5 sec (24.0 batch per second; 1.1 sec remained)\n",
      "batch 200 of 206 for 8.5 sec (23.6 batch per second; 0.3 sec remained)\n",
      "epoch 0, test: unit lossis 9.222, peplexity: 10119.013\n",
      "382.94sec (392.41sec total)\n",
      "epoch 1 of 13\n",
      "batch 232 of 2323 for 37.7 sec (6.2 batch per second; 339.6 sec remained)\n",
      "batch 464 of 2323 for 75.9 sec (6.1 batch per second; 304.1 sec remained)\n",
      "batch 696 of 2323 for 114.7 sec (6.1 batch per second; 268.0 sec remained)\n",
      "batch 928 of 2323 for 153.6 sec (6.0 batch per second; 230.9 sec remained)\n",
      "batch 1160 of 2323 for 192.7 sec (6.0 batch per second; 193.2 sec remained)\n",
      "batch 1392 of 2323 for 230.5 sec (6.0 batch per second; 154.1 sec remained)\n",
      "batch 1624 of 2323 for 259.4 sec (6.3 batch per second; 111.7 sec remained)\n",
      "batch 1856 of 2323 for 295.4 sec (6.3 batch per second; 74.3 sec remained)\n",
      "batch 2088 of 2323 for 333.0 sec (6.3 batch per second; 37.5 sec remained)\n",
      "batch 2320 of 2323 for 371.1 sec (6.3 batch per second; 0.5 sec remained)\n",
      "epoch 1, train: unit lossis 9.221, peplexity: 10110.956\n",
      "batch 20 of 206 for 1.0 sec (20.4 batch per second; 9.1 sec remained)\n",
      "batch 40 of 206 for 1.8 sec (22.8 batch per second; 7.3 sec remained)\n",
      "batch 60 of 206 for 2.6 sec (23.4 batch per second; 6.2 sec remained)\n",
      "batch 80 of 206 for 3.4 sec (23.6 batch per second; 5.3 sec remained)\n",
      "batch 100 of 206 for 4.3 sec (23.5 batch per second; 4.5 sec remained)\n",
      "batch 120 of 206 for 5.1 sec (23.3 batch per second; 3.7 sec remained)\n",
      "batch 140 of 206 for 6.1 sec (23.0 batch per second; 2.9 sec remained)\n",
      "batch 160 of 206 for 7.0 sec (22.7 batch per second; 2.0 sec remained)\n",
      "batch 180 of 206 for 8.0 sec (22.4 batch per second; 1.2 sec remained)\n",
      "batch 200 of 206 for 9.1 sec (22.0 batch per second; 0.3 sec remained)\n",
      "epoch 1, test: unit lossis 9.222, peplexity: 10119.013\n",
      "380.74sec (773.14sec total)\n",
      "epoch 2 of 13\n",
      "batch 232 of 2323 for 37.5 sec (6.2 batch per second; 338.1 sec remained)\n",
      "batch 464 of 2323 for 75.4 sec (6.2 batch per second; 302.1 sec remained)\n",
      "batch 696 of 2323 for 114.0 sec (6.1 batch per second; 266.5 sec remained)\n",
      "batch 928 of 2323 for 152.8 sec (6.1 batch per second; 229.7 sec remained)\n",
      "batch 1160 of 2323 for 192.0 sec (6.0 batch per second; 192.5 sec remained)\n",
      "batch 1392 of 2323 for 231.2 sec (6.0 batch per second; 154.6 sec remained)\n",
      "batch 1624 of 2323 for 261.0 sec (6.2 batch per second; 112.4 sec remained)\n",
      "batch 1856 of 2323 for 295.8 sec (6.3 batch per second; 74.4 sec remained)\n",
      "batch 2088 of 2323 for 332.7 sec (6.3 batch per second; 37.4 sec remained)\n",
      "batch 2320 of 2323 for 370.8 sec (6.3 batch per second; 0.5 sec remained)\n",
      "epoch 2, train: unit lossis 9.221, peplexity: 10110.955\n",
      "batch 20 of 206 for 1.0 sec (19.7 batch per second; 9.4 sec remained)\n",
      "batch 40 of 206 for 1.9 sec (21.5 batch per second; 7.7 sec remained)\n",
      "batch 60 of 206 for 2.7 sec (22.1 batch per second; 6.6 sec remained)\n",
      "batch 80 of 206 for 3.6 sec (22.2 batch per second; 5.7 sec remained)\n",
      "batch 100 of 206 for 4.6 sec (22.0 batch per second; 4.8 sec remained)\n",
      "batch 120 of 206 for 5.5 sec (21.8 batch per second; 4.0 sec remained)\n",
      "batch 140 of 206 for 6.5 sec (21.5 batch per second; 3.1 sec remained)\n",
      "batch 160 of 206 for 7.6 sec (21.1 batch per second; 2.2 sec remained)\n",
      "batch 180 of 206 for 8.7 sec (20.7 batch per second; 1.3 sec remained)\n",
      "batch 200 of 206 for 9.9 sec (20.3 batch per second; 0.3 sec remained)\n",
      "epoch 2, test: unit lossis 9.222, peplexity: 10119.013\n",
      "381.18sec (1154.32sec total)\n",
      "epoch 3 of 13\n",
      "batch 232 of 2323 for 37.0 sec (6.3 batch per second; 333.9 sec remained)\n",
      "batch 464 of 2323 for 74.9 sec (6.2 batch per second; 300.3 sec remained)\n",
      "batch 696 of 2323 for 113.1 sec (6.2 batch per second; 264.5 sec remained)\n",
      "batch 928 of 2323 for 151.7 sec (6.1 batch per second; 228.0 sec remained)\n",
      "batch 1160 of 2323 for 190.6 sec (6.1 batch per second; 191.1 sec remained)\n",
      "batch 1392 of 2323 for 229.9 sec (6.1 batch per second; 153.8 sec remained)\n",
      "batch 1624 of 2323 for 263.4 sec (6.2 batch per second; 113.4 sec remained)\n",
      "batch 1856 of 2323 for 295.9 sec (6.3 batch per second; 74.5 sec remained)\n",
      "batch 2088 of 2323 for 332.5 sec (6.3 batch per second; 37.4 sec remained)\n",
      "batch 2320 of 2323 for 370.1 sec (6.3 batch per second; 0.5 sec remained)\n",
      "epoch 3, train: unit lossis 9.221, peplexity: 10110.956\n",
      "batch 20 of 206 for 1.1 sec (18.7 batch per second; 9.9 sec remained)\n",
      "batch 40 of 206 for 2.0 sec (20.3 batch per second; 8.2 sec remained)\n",
      "batch 60 of 206 for 2.9 sec (20.7 batch per second; 7.0 sec remained)\n",
      "batch 80 of 206 for 3.9 sec (20.8 batch per second; 6.1 sec remained)\n",
      "batch 100 of 206 for 4.9 sec (20.6 batch per second; 5.1 sec remained)\n",
      "batch 120 of 206 for 5.9 sec (20.3 batch per second; 4.2 sec remained)\n",
      "batch 140 of 206 for 7.0 sec (19.9 batch per second; 3.3 sec remained)\n",
      "batch 160 of 206 for 8.2 sec (19.5 batch per second; 2.4 sec remained)\n",
      "batch 180 of 206 for 9.4 sec (19.1 batch per second; 1.4 sec remained)\n",
      "batch 200 of 206 for 10.8 sec (18.6 batch per second; 0.3 sec remained)\n",
      "epoch 3, test: unit lossis 9.222, peplexity: 10119.013\n",
      "381.47sec (1535.79sec total)\n",
      "epoch 4 of 13\n",
      "batch 232 of 2323 for 36.5 sec (6.4 batch per second; 328.5 sec remained)\n",
      "batch 464 of 2323 for 73.9 sec (6.3 batch per second; 296.3 sec remained)\n",
      "batch 696 of 2323 for 111.8 sec (6.2 batch per second; 261.3 sec remained)\n",
      "batch 928 of 2323 for 150.5 sec (6.2 batch per second; 226.3 sec remained)\n",
      "batch 1160 of 2323 for 189.6 sec (6.1 batch per second; 190.1 sec remained)\n",
      "batch 1392 of 2323 for 228.6 sec (6.1 batch per second; 152.9 sec remained)\n",
      "batch 1624 of 2323 for 267.7 sec (6.1 batch per second; 115.2 sec remained)\n",
      "batch 1856 of 2323 for 295.7 sec (6.3 batch per second; 74.4 sec remained)\n",
      "batch 2088 of 2323 for 331.6 sec (6.3 batch per second; 37.3 sec remained)\n",
      "batch 2320 of 2323 for 368.7 sec (6.3 batch per second; 0.5 sec remained)\n",
      "epoch 4, train: unit lossis 9.221, peplexity: 10110.956\n",
      "batch 20 of 206 for 1.1 sec (17.6 batch per second; 10.6 sec remained)\n",
      "batch 40 of 206 for 2.1 sec (19.4 batch per second; 8.6 sec remained)\n",
      "batch 60 of 206 for 3.1 sec (19.6 batch per second; 7.4 sec remained)\n",
      "batch 80 of 206 for 4.1 sec (19.6 batch per second; 6.4 sec remained)\n",
      "batch 100 of 206 for 5.2 sec (19.4 batch per second; 5.5 sec remained)\n",
      "batch 120 of 206 for 6.3 sec (19.2 batch per second; 4.5 sec remained)\n",
      "batch 140 of 206 for 7.5 sec (18.8 batch per second; 3.5 sec remained)\n",
      "batch 160 of 206 for 8.7 sec (18.3 batch per second; 2.5 sec remained)\n",
      "batch 180 of 206 for 10.1 sec (17.8 batch per second; 1.5 sec remained)\n",
      "batch 200 of 206 for 11.7 sec (17.2 batch per second; 0.3 sec remained)\n",
      "epoch 4, test: unit lossis 9.222, peplexity: 10119.013\n",
      "381.02sec (1916.81sec total)\n",
      "epoch 5 of 13\n",
      "batch 232 of 2323 for 35.6 sec (6.5 batch per second; 320.6 sec remained)\n",
      "batch 464 of 2323 for 72.8 sec (6.4 batch per second; 291.7 sec remained)\n",
      "batch 696 of 2323 for 108.9 sec (6.4 batch per second; 254.5 sec remained)\n",
      "batch 928 of 2323 for 147.4 sec (6.3 batch per second; 221.6 sec remained)\n",
      "batch 1160 of 2323 for 186.5 sec (6.2 batch per second; 187.0 sec remained)\n",
      "batch 1392 of 2323 for 225.7 sec (6.2 batch per second; 150.9 sec remained)\n",
      "batch 1624 of 2323 for 265.2 sec (6.1 batch per second; 114.1 sec remained)\n",
      "batch 1856 of 2323 for 293.3 sec (6.3 batch per second; 73.8 sec remained)\n",
      "batch 2088 of 2323 for 328.9 sec (6.3 batch per second; 37.0 sec remained)\n",
      "batch 2320 of 2323 for 366.3 sec (6.3 batch per second; 0.5 sec remained)\n",
      "epoch 5, train: unit lossis 5.767, peplexity: 319.705\n",
      "batch 20 of 206 for 1.1 sec (17.5 batch per second; 10.6 sec remained)\n",
      "batch 40 of 206 for 2.1 sec (19.4 batch per second; 8.6 sec remained)\n",
      "batch 60 of 206 for 3.1 sec (19.5 batch per second; 7.5 sec remained)\n",
      "batch 80 of 206 for 4.1 sec (19.7 batch per second; 6.4 sec remained)\n",
      "batch 100 of 206 for 5.1 sec (19.5 batch per second; 5.4 sec remained)\n",
      "batch 120 of 206 for 6.2 sec (19.3 batch per second; 4.5 sec remained)\n",
      "batch 140 of 206 for 7.3 sec (19.1 batch per second; 3.5 sec remained)\n",
      "batch 160 of 206 for 8.6 sec (18.6 batch per second; 2.5 sec remained)\n",
      "batch 180 of 206 for 10.0 sec (18.0 batch per second; 1.4 sec remained)\n",
      "batch 200 of 206 for 11.4 sec (17.5 batch per second; 0.3 sec remained)\n",
      "epoch 5, test: unit lossis 5.317, peplexity: 203.817\n",
      "378.67sec (2295.48sec total)\n",
      "epoch 6 of 13\n",
      "batch 232 of 2323 for 35.8 sec (6.5 batch per second; 322.2 sec remained)\n",
      "batch 464 of 2323 for 72.6 sec (6.4 batch per second; 290.9 sec remained)\n",
      "batch 696 of 2323 for 110.4 sec (6.3 batch per second; 258.1 sec remained)\n",
      "batch 928 of 2323 for 149.2 sec (6.2 batch per second; 224.2 sec remained)\n",
      "batch 1160 of 2323 for 188.2 sec (6.2 batch per second; 188.7 sec remained)\n",
      "batch 1392 of 2323 for 227.6 sec (6.1 batch per second; 152.2 sec remained)\n",
      "batch 1624 of 2323 for 266.7 sec (6.1 batch per second; 114.8 sec remained)\n",
      "batch 1856 of 2323 for 299.1 sec (6.2 batch per second; 75.3 sec remained)\n",
      "batch 2088 of 2323 for 332.8 sec (6.3 batch per second; 37.5 sec remained)\n",
      "batch 2320 of 2323 for 369.3 sec (6.3 batch per second; 0.5 sec remained)\n",
      "epoch 6, train: unit lossis 5.011, peplexity: 150.069\n",
      "batch 20 of 206 for 1.3 sec (15.6 batch per second; 11.9 sec remained)\n",
      "batch 40 of 206 for 2.4 sec (16.8 batch per second; 9.9 sec remained)\n",
      "batch 60 of 206 for 3.5 sec (17.1 batch per second; 8.5 sec remained)\n",
      "batch 80 of 206 for 4.7 sec (17.0 batch per second; 7.4 sec remained)\n",
      "batch 100 of 206 for 6.0 sec (16.6 batch per second; 6.4 sec remained)\n",
      "batch 120 of 206 for 7.4 sec (16.2 batch per second; 5.3 sec remained)\n",
      "batch 140 of 206 for 8.9 sec (15.7 batch per second; 4.2 sec remained)\n",
      "batch 160 of 206 for 10.7 sec (14.9 batch per second; 3.1 sec remained)\n",
      "batch 180 of 206 for 12.8 sec (14.0 batch per second; 1.9 sec remained)\n",
      "batch 200 of 206 for 15.6 sec (12.8 batch per second; 0.5 sec remained)\n",
      "epoch 6, test: unit lossis 5.043, peplexity: 154.901\n",
      "386.39sec (2681.87sec total)\n",
      "epoch 7 of 13\n",
      "batch 232 of 2323 for 31.4 sec (7.4 batch per second; 283.2 sec remained)\n",
      "batch 464 of 2323 for 67.6 sec (6.9 batch per second; 271.0 sec remained)\n",
      "batch 696 of 2323 for 105.0 sec (6.6 batch per second; 245.6 sec remained)\n",
      "batch 928 of 2323 for 143.4 sec (6.5 batch per second; 215.6 sec remained)\n",
      "batch 1160 of 2323 for 182.0 sec (6.4 batch per second; 182.5 sec remained)\n",
      "batch 1392 of 2323 for 221.0 sec (6.3 batch per second; 147.8 sec remained)\n",
      "batch 1624 of 2323 for 260.3 sec (6.2 batch per second; 112.1 sec remained)\n",
      "batch 1856 of 2323 for 299.6 sec (6.2 batch per second; 75.4 sec remained)\n",
      "batch 2088 of 2323 for 328.0 sec (6.4 batch per second; 36.9 sec remained)\n",
      "batch 2320 of 2323 for 363.2 sec (6.4 batch per second; 0.5 sec remained)\n",
      "epoch 7, train: unit lossis 4.772, peplexity: 118.126\n",
      "batch 20 of 206 for 1.4 sec (14.4 batch per second; 12.9 sec remained)\n",
      "batch 40 of 206 for 2.6 sec (15.6 batch per second; 10.7 sec remained)\n",
      "batch 60 of 206 for 3.9 sec (15.6 batch per second; 9.4 sec remained)\n",
      "batch 80 of 206 for 5.2 sec (15.4 batch per second; 8.2 sec remained)\n",
      "batch 100 of 206 for 6.7 sec (14.9 batch per second; 7.1 sec remained)\n",
      "batch 120 of 206 for 8.4 sec (14.2 batch per second; 6.1 sec remained)\n",
      "batch 140 of 206 for 10.4 sec (13.5 batch per second; 4.9 sec remained)\n",
      "batch 160 of 206 for 12.9 sec (12.4 batch per second; 3.7 sec remained)\n",
      "batch 180 of 206 for 17.5 sec (10.3 batch per second; 2.5 sec remained)\n",
      "batch 200 of 206 for 20.3 sec (9.9 batch per second; 0.6 sec remained)\n",
      "epoch 7, test: unit lossis 4.938, peplexity: 139.470\n",
      "384.71sec (3066.59sec total)\n",
      "epoch 8 of 13\n",
      "batch 232 of 2323 for 29.2 sec (7.9 batch per second; 263.1 sec remained)\n",
      "batch 464 of 2323 for 64.2 sec (7.2 batch per second; 257.0 sec remained)\n",
      "batch 696 of 2323 for 101.1 sec (6.9 batch per second; 236.3 sec remained)\n",
      "batch 928 of 2323 for 139.2 sec (6.7 batch per second; 209.2 sec remained)\n",
      "batch 1160 of 2323 for 177.7 sec (6.5 batch per second; 178.2 sec remained)\n",
      "batch 1392 of 2323 for 216.7 sec (6.4 batch per second; 144.9 sec remained)\n",
      "batch 1624 of 2323 for 255.8 sec (6.3 batch per second; 110.1 sec remained)\n",
      "batch 1856 of 2323 for 295.1 sec (6.3 batch per second; 74.2 sec remained)\n",
      "batch 2088 of 2323 for 324.6 sec (6.4 batch per second; 36.5 sec remained)\n",
      "batch 2320 of 2323 for 359.7 sec (6.4 batch per second; 0.5 sec remained)\n",
      "epoch 8, train: unit lossis 4.654, peplexity: 105.005\n",
      "batch 20 of 206 for 1.6 sec (12.5 batch per second; 14.8 sec remained)\n",
      "batch 40 of 206 for 3.0 sec (13.5 batch per second; 12.3 sec remained)\n",
      "batch 60 of 206 for 4.4 sec (13.5 batch per second; 10.8 sec remained)\n",
      "batch 80 of 206 for 6.1 sec (13.1 batch per second; 9.6 sec remained)\n",
      "batch 100 of 206 for 8.0 sec (12.5 batch per second; 8.5 sec remained)\n",
      "batch 120 of 206 for 10.3 sec (11.6 batch per second; 7.4 sec remained)\n",
      "batch 140 of 206 for 13.8 sec (10.2 batch per second; 6.5 sec remained)\n",
      "batch 160 of 206 for 18.4 sec (8.7 batch per second; 5.3 sec remained)\n",
      "batch 180 of 206 for 19.0 sec (9.5 batch per second; 2.7 sec remained)\n",
      "batch 200 of 206 for 19.5 sec (10.2 batch per second; 0.6 sec remained)\n",
      "epoch 8, test: unit lossis 4.892, peplexity: 133.205\n",
      "379.84sec (3446.42sec total)\n",
      "epoch 9 of 13\n",
      "batch 232 of 2323 for 31.8 sec (7.3 batch per second; 286.6 sec remained)\n",
      "batch 464 of 2323 for 65.6 sec (7.1 batch per second; 262.8 sec remained)\n",
      "batch 696 of 2323 for 102.3 sec (6.8 batch per second; 239.2 sec remained)\n",
      "batch 928 of 2323 for 140.2 sec (6.6 batch per second; 210.8 sec remained)\n",
      "batch 1160 of 2323 for 178.7 sec (6.5 batch per second; 179.2 sec remained)\n",
      "batch 1392 of 2323 for 217.4 sec (6.4 batch per second; 145.4 sec remained)\n",
      "batch 1624 of 2323 for 256.4 sec (6.3 batch per second; 110.3 sec remained)\n",
      "batch 1856 of 2323 for 295.5 sec (6.3 batch per second; 74.4 sec remained)\n",
      "batch 2088 of 2323 for 332.0 sec (6.3 batch per second; 37.4 sec remained)\n",
      "batch 2320 of 2323 for 362.2 sec (6.4 batch per second; 0.5 sec remained)\n",
      "epoch 9, train: unit lossis 4.593, peplexity: 98.774\n",
      "batch 20 of 206 for 2.0 sec (9.8 batch per second; 19.0 sec remained)\n",
      "batch 40 of 206 for 3.9 sec (10.4 batch per second; 16.0 sec remained)\n",
      "batch 60 of 206 for 6.1 sec (9.9 batch per second; 14.8 sec remained)\n",
      "batch 80 of 206 for 9.1 sec (8.8 batch per second; 14.3 sec remained)\n",
      "batch 100 of 206 for 14.7 sec (6.8 batch per second; 15.6 sec remained)\n",
      "batch 120 of 206 for 15.3 sec (7.9 batch per second; 10.9 sec remained)\n",
      "batch 140 of 206 for 15.8 sec (8.8 batch per second; 7.5 sec remained)\n",
      "batch 160 of 206 for 16.4 sec (9.8 batch per second; 4.7 sec remained)\n",
      "batch 180 of 206 for 17.0 sec (10.6 batch per second; 2.5 sec remained)\n",
      "batch 200 of 206 for 17.5 sec (11.4 batch per second; 0.5 sec remained)\n",
      "epoch 9, test: unit lossis 4.870, peplexity: 130.340\n",
      "380.40sec (3826.82sec total)\n",
      "epoch 10 of 13\n",
      "batch 232 of 2323 for 39.4 sec (5.9 batch per second; 355.4 sec remained)\n",
      "batch 464 of 2323 for 68.1 sec (6.8 batch per second; 272.7 sec remained)\n",
      "batch 696 of 2323 for 103.8 sec (6.7 batch per second; 242.7 sec remained)\n",
      "batch 928 of 2323 for 138.2 sec (6.7 batch per second; 207.8 sec remained)\n",
      "batch 1160 of 2323 for 176.8 sec (6.6 batch per second; 177.3 sec remained)\n",
      "batch 1392 of 2323 for 215.6 sec (6.5 batch per second; 144.2 sec remained)\n",
      "batch 1624 of 2323 for 254.7 sec (6.4 batch per second; 109.6 sec remained)\n",
      "batch 1856 of 2323 for 294.0 sec (6.3 batch per second; 74.0 sec remained)\n",
      "batch 2088 of 2323 for 328.7 sec (6.4 batch per second; 37.0 sec remained)\n",
      "batch 2320 of 2323 for 360.3 sec (6.4 batch per second; 0.5 sec remained)\n",
      "epoch 10, train: unit lossis 4.561, peplexity: 95.687\n",
      "batch 20 of 206 for 2.0 sec (10.2 batch per second; 18.2 sec remained)\n",
      "batch 40 of 206 for 3.7 sec (10.8 batch per second; 15.3 sec remained)\n",
      "batch 60 of 206 for 5.8 sec (10.4 batch per second; 14.0 sec remained)\n",
      "batch 80 of 206 for 8.5 sec (9.4 batch per second; 13.4 sec remained)\n",
      "batch 100 of 206 for 14.9 sec (6.7 batch per second; 15.8 sec remained)\n",
      "batch 120 of 206 for 15.5 sec (7.7 batch per second; 11.1 sec remained)\n",
      "batch 140 of 206 for 16.1 sec (8.7 batch per second; 7.6 sec remained)\n",
      "batch 160 of 206 for 16.6 sec (9.6 batch per second; 4.8 sec remained)\n",
      "batch 180 of 206 for 17.2 sec (10.5 batch per second; 2.5 sec remained)\n",
      "batch 200 of 206 for 17.7 sec (11.3 batch per second; 0.5 sec remained)\n",
      "epoch 10, test: unit lossis 4.858, peplexity: 128.811\n",
      "378.69sec (4205.52sec total)\n",
      "epoch 11 of 13\n",
      "batch 232 of 2323 for 39.6 sec (5.9 batch per second; 356.8 sec remained)\n",
      "batch 464 of 2323 for 68.2 sec (6.8 batch per second; 273.2 sec remained)\n",
      "batch 696 of 2323 for 103.9 sec (6.7 batch per second; 242.8 sec remained)\n",
      "batch 928 of 2323 for 140.9 sec (6.6 batch per second; 211.7 sec remained)\n",
      "batch 1160 of 2323 for 178.7 sec (6.5 batch per second; 179.2 sec remained)\n",
      "batch 1392 of 2323 for 217.4 sec (6.4 batch per second; 145.4 sec remained)\n",
      "batch 1624 of 2323 for 256.0 sec (6.3 batch per second; 110.2 sec remained)\n",
      "batch 1856 of 2323 for 295.2 sec (6.3 batch per second; 74.3 sec remained)\n",
      "batch 2088 of 2323 for 334.6 sec (6.2 batch per second; 37.7 sec remained)\n",
      "batch 2320 of 2323 for 363.7 sec (6.4 batch per second; 0.5 sec remained)\n",
      "epoch 11, train: unit lossis 4.545, peplexity: 94.129\n",
      "batch 20 of 206 for 2.6 sec (7.6 batch per second; 24.6 sec remained)\n",
      "batch 40 of 206 for 5.4 sec (7.3 batch per second; 22.6 sec remained)\n",
      "batch 60 of 206 for 11.6 sec (5.2 batch per second; 28.3 sec remained)\n",
      "batch 80 of 206 for 12.2 sec (6.6 batch per second; 19.2 sec remained)\n",
      "batch 100 of 206 for 12.8 sec (7.8 batch per second; 13.5 sec remained)\n",
      "batch 120 of 206 for 13.3 sec (9.0 batch per second; 9.5 sec remained)\n",
      "batch 140 of 206 for 13.9 sec (10.1 batch per second; 6.5 sec remained)\n",
      "batch 160 of 206 for 14.5 sec (11.1 batch per second; 4.2 sec remained)\n",
      "batch 180 of 206 for 15.0 sec (12.0 batch per second; 2.2 sec remained)\n",
      "batch 200 of 206 for 15.6 sec (12.8 batch per second; 0.5 sec remained)\n",
      "epoch 11, test: unit lossis 4.852, peplexity: 127.947\n",
      "380.10sec (4585.62sec total)\n",
      "epoch 12 of 13\n",
      "batch 232 of 2323 for 39.5 sec (5.9 batch per second; 356.3 sec remained)\n",
      "batch 464 of 2323 for 70.8 sec (6.6 batch per second; 283.7 sec remained)\n",
      "batch 696 of 2323 for 104.8 sec (6.6 batch per second; 245.0 sec remained)\n",
      "batch 928 of 2323 for 141.4 sec (6.6 batch per second; 212.6 sec remained)\n",
      "batch 1160 of 2323 for 179.2 sec (6.5 batch per second; 179.7 sec remained)\n",
      "batch 1392 of 2323 for 217.5 sec (6.4 batch per second; 145.4 sec remained)\n",
      "batch 1624 of 2323 for 256.2 sec (6.3 batch per second; 110.3 sec remained)\n",
      "batch 1856 of 2323 for 295.1 sec (6.3 batch per second; 74.3 sec remained)\n",
      "batch 2088 of 2323 for 334.5 sec (6.2 batch per second; 37.6 sec remained)\n",
      "batch 2320 of 2323 for 367.6 sec (6.3 batch per second; 0.5 sec remained)\n",
      "epoch 12, train: unit lossis 4.536, peplexity: 93.328\n",
      "batch 20 of 206 for 6.2 sec (3.2 batch per second; 57.4 sec remained)\n",
      "batch 40 of 206 for 6.7 sec (5.9 batch per second; 27.9 sec remained)\n",
      "batch 60 of 206 for 7.3 sec (8.2 batch per second; 17.8 sec remained)\n",
      "batch 80 of 206 for 7.9 sec (10.2 batch per second; 12.4 sec remained)\n",
      "batch 100 of 206 for 8.4 sec (11.8 batch per second; 9.0 sec remained)\n",
      "batch 120 of 206 for 9.0 sec (13.3 batch per second; 6.5 sec remained)\n",
      "batch 140 of 206 for 9.6 sec (14.6 batch per second; 4.5 sec remained)\n",
      "batch 160 of 206 for 10.2 sec (15.7 batch per second; 2.9 sec remained)\n",
      "batch 180 of 206 for 10.8 sec (16.7 batch per second; 1.6 sec remained)\n",
      "batch 200 of 206 for 11.4 sec (17.6 batch per second; 0.3 sec remained)\n",
      "epoch 12, test: unit lossis 4.848, peplexity: 127.467\n",
      "380.28sec (4965.90sec total)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.models.rnn.ptb import reader\n",
    "from mutils.tick import tick\n",
    "\n",
    "def run():\n",
    "    tf.reset_default_graph()\n",
    "    tick.tack(restart=True)\n",
    "    \n",
    "    config = SmallConfig\n",
    "    eval_config = EvaluationConfig\n",
    "    \n",
    "    X_p = tf.placeholder(tf.int32, [config.batch_size, config.num_steps])\n",
    "    Y_p = tf.placeholder(tf.int32, [config.batch_size, config.num_steps])\n",
    "\n",
    "    print 'building model'\n",
    "    ## to turn on GPU support install GPU version of tensorflow (see links)\n",
    "    with tf.device('/gpu:0'):\n",
    "        initializer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)\n",
    "        with tf.variable_scope(\"model\", reuse=None, initializer=initializer):\n",
    "            train_model = build_model(X_p, Y_p, config)\n",
    "        ## same model that shares weights (!) but\n",
    "        ## does _not_ update anything and takes single input\n",
    "        with tf.variable_scope(\"model\", reuse=True, initializer=initializer):\n",
    "            X_p_a = tf.placeholder(tf.int32, [eval_config.batch_size, eval_config.num_steps])\n",
    "            Y_p_a = tf.placeholder(tf.int32, [eval_config.batch_size, eval_config.num_steps])\n",
    "            apply_model = build_model(X_p_a, Y_p_a, eval_config, is_training=False)\n",
    "\n",
    "    tick.tack()\n",
    "    print 'reading data'\n",
    "    raw_data = reader.ptb_raw_data(config.data_path)\n",
    "    data = record('Data', {\n",
    "            'train': raw_data[0],\n",
    "            'test': raw_data[2]\n",
    "        })\n",
    "    \n",
    "    tick.tack()\n",
    "    print 'training'\n",
    "    try:\n",
    "        train((X_p, Y_p), train_model, data)\n",
    "    except tf.errors.InvalidArgumentError as e:\n",
    "        print 'ERROR: You probably dont have the specified device'\n",
    "        print e\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links\n",
    "\n",
    "API:\n",
    "\n",
    "* Understanding [tf.get_variable()](https://www.tensorflow.org/versions/r0.7/how_tos/variable_scope/index.html#understanding-tf-get-variable)\n",
    "* RNN cell [source](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py)\n",
    "* [tf.device](https://www.tensorflow.org/versions/r0.7/api_docs/python/framework.html#Graph.device)\n",
    "\n",
    "GPU:\n",
    "* https://www.tensorflow.org/versions/r0.7/how_tos/using_gpu/index.html#using-gpus\n",
    "* https://www.tensorflow.org/versions/r0.7/get_started/os_setup.html#optional-linux-enable-gpu-support"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
